"""
COMPLETE TRANSFORMER MODEL
M√¥ h√¨nh Transformer ho√†n ch·ªânh cho d·ªãch m√°y Seq2Seq
"""

import torch
import torch.nn as nn
from transformer_encoder_decoder import (
    Encoder, Decoder,
    create_padding_mask, create_target_mask
)

# ============================================================================
# TRANSFORMER MODEL
# ============================================================================

class Transformer(nn.Module):
    """
    M√¥ h√¨nh Transformer ho√†n ch·ªânh cho Neural Machine Translation
    
    Args:
        src_vocab_size: K√≠ch th∆∞·ªõc vocabulary source language
        tgt_vocab_size: K√≠ch th∆∞·ªõc vocabulary target language
        d_model: Dimension c·ªßa model (m·∫∑c ƒë·ªãnh 512)
        n_layers: S·ªë l∆∞·ª£ng encoder/decoder layers (m·∫∑c ƒë·ªãnh 6)
        n_heads: S·ªë l∆∞·ª£ng attention heads (m·∫∑c ƒë·ªãnh 8)
        d_ff: Dimension c·ªßa feed-forward network (m·∫∑c ƒë·ªãnh 2048)
        dropout: Dropout rate (m·∫∑c ƒë·ªãnh 0.1)
        max_len: Maximum sequence length (m·∫∑c ƒë·ªãnh 5000)
        pad_idx: Index c·ªßa padding token (m·∫∑c ƒë·ªãnh 0)
    """
    def __init__(
        self,
        src_vocab_size,
        tgt_vocab_size,
        d_model=512,
        n_layers=6,
        n_heads=8,
        d_ff=2048,
        dropout=0.1,
        max_len=5000,
        pad_idx=0
    ):
        super().__init__()
        
        self.pad_idx = pad_idx
        
        # Encoder
        self.encoder = Encoder(
            vocab_size=src_vocab_size,
            d_model=d_model,
            n_layers=n_layers,
            n_heads=n_heads,
            d_ff=d_ff,
            dropout=dropout,
            max_len=max_len
        )
        
        # Decoder
        self.decoder = Decoder(
            vocab_size=tgt_vocab_size,
            d_model=d_model,
            n_layers=n_layers,
            n_heads=n_heads,
            d_ff=d_ff,
            dropout=dropout,
            max_len=max_len
        )
        
        # Kh·ªüi t·∫°o weights
        self._init_weights()
        
    def _init_weights(self):
        """
        Kh·ªüi t·∫°o weights theo Xavier Uniform
        """
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)
    
    def forward(self, src, tgt):
        """
        Forward pass
        
        Args:
            src: Source sequence [batch_size, src_len]
            tgt: Target sequence [batch_size, tgt_len]
            
        Returns:
            output: Logits [batch_size, tgt_len, tgt_vocab_size]
        """
        # T·∫°o masks
        src_mask = create_padding_mask(src, self.pad_idx)
        tgt_mask = create_target_mask(tgt, self.pad_idx)
        
        # Encoder
        encoder_output = self.encoder(src, src_mask)
        
        # Decoder
        output = self.decoder(tgt, encoder_output, src_mask, tgt_mask)
        
        return output
    
    def encode(self, src):
        """
        Ch·ªâ ch·∫°y encoder (d√πng khi inference)
        
        Args:
            src: Source sequence [batch_size, src_len]
            
        Returns:
            encoder_output: [batch_size, src_len, d_model]
            src_mask: [batch_size, 1, 1, src_len]
        """
        src_mask = create_padding_mask(src, self.pad_idx)
        encoder_output = self.encoder(src, src_mask)
        return encoder_output, src_mask
    
    def decode(self, tgt, encoder_output, src_mask):
        """
        Ch·ªâ ch·∫°y decoder (d√πng khi inference)
        
        Args:
            tgt: Target sequence [batch_size, tgt_len]
            encoder_output: Encoder output [batch_size, src_len, d_model]
            src_mask: Source mask [batch_size, 1, 1, src_len]
            
        Returns:
            output: Logits [batch_size, tgt_len, tgt_vocab_size]
        """
        tgt_mask = create_target_mask(tgt, self.pad_idx)
        output = self.decoder(tgt, encoder_output, src_mask, tgt_mask)
        return output

# ============================================================================
# MODEL CONFIGURATION
# ============================================================================

def get_model_config(model_size='base'):
    """
    Tr·∫£ v·ªÅ config cho c√°c k√≠ch th∆∞·ªõc model kh√°c nhau
    
    Args:
        model_size: 'tiny', 'small', 'base', 'large'
        
    Returns:
        config: Dictionary ch·ª©a hyperparameters
    """
    configs = {
        'tiny': {
            'd_model': 256,
            'n_layers': 2,
            'n_heads': 4,
            'd_ff': 1024,
            'dropout': 0.1
        },
        'small': {
            'd_model': 256,
            'n_layers': 4,
            'n_heads': 8,
            'd_ff': 1024,
            'dropout': 0.1
        },
        'base': {
            'd_model': 512,
            'n_layers': 6,
            'n_heads': 8,
            'd_ff': 2048,
            'dropout': 0.1
        },
        'large': {
            'd_model': 1024,
            'n_layers': 6,
            'n_heads': 16,
            'd_ff': 4096,
            'dropout': 0.1
        }
    }
    
    return configs.get(model_size, configs['base'])

def create_model(src_vocab_size, tgt_vocab_size, model_size='base', pad_idx=0):
    """
    T·∫°o Transformer model v·ªõi config ƒë√£ ch·ªçn
    
    Args:
        src_vocab_size: K√≠ch th∆∞·ªõc source vocabulary
        tgt_vocab_size: K√≠ch th∆∞·ªõc target vocabulary
        model_size: K√≠ch th∆∞·ªõc model ('tiny', 'small', 'base', 'large')
        pad_idx: Padding index
        
    Returns:
        model: Transformer model
        config: Model configuration
    """
    config = get_model_config(model_size)
    
    model = Transformer(
        src_vocab_size=src_vocab_size,
        tgt_vocab_size=tgt_vocab_size,
        d_model=config['d_model'],
        n_layers=config['n_layers'],
        n_heads=config['n_heads'],
        d_ff=config['d_ff'],
        dropout=config['dropout'],
        pad_idx=pad_idx
    )
    
    return model, config

# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

def count_parameters(model):
    """
    ƒê·∫øm s·ªë l∆∞·ª£ng parameters c·ªßa model
    
    Args:
        model: PyTorch model
        
    Returns:
        total: T·ªïng s·ªë parameters
        trainable: S·ªë parameters c√≥ th·ªÉ train
    """
    total = sum(p.numel() for p in model.parameters())
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    return total, trainable

def print_model_info(model, model_size='base'):
    """
    In th√¥ng tin v·ªÅ model
    """
    total_params, trainable_params = count_parameters(model)
    
    print("="*70)
    print("TH√îNG TIN M√î H√åNH TRANSFORMER")
    print("="*70)
    print(f"\nK√≠ch th∆∞·ªõc model: {model_size.upper()}")
    print(f"\nS·ªë l∆∞·ª£ng parameters:")
    print(f"  - Total: {total_params:,}")
    print(f"  - Trainable: {trainable_params:,}")
    print(f"  - Model size: ~{total_params * 4 / (1024**2):.2f} MB (float32)")
    
    config = get_model_config(model_size)
    print(f"\nC·∫•u h√¨nh:")
    print(f"  - d_model: {config['d_model']}")
    print(f"  - n_layers: {config['n_layers']}")
    print(f"  - n_heads: {config['n_heads']}")
    print(f"  - d_ff: {config['d_ff']}")
    print(f"  - dropout: {config['dropout']}")
    print("="*70)

# ============================================================================
# TEST COMPLETE MODEL
# ============================================================================

if __name__ == "__main__":
    print("="*70)
    print("KI·ªÇM TRA TRANSFORMER MODEL HO√ÄN CH·ªàNH")
    print("="*70)
    
    # Hyperparameters
    src_vocab_size = 10000
    tgt_vocab_size = 8000
    batch_size = 4
    src_len = 15
    tgt_len = 20
    pad_idx = 0
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nDevice: {device}\n")
    
    # Test v·ªõi c√°c k√≠ch th∆∞·ªõc model kh√°c nhau
    for model_size in ['tiny', 'small', 'base']:
        print(f"\n{'='*70}")
        print(f"TEST MODEL SIZE: {model_size.upper()}")
        print(f"{'='*70}\n")
        
        # T·∫°o model
        model, config = create_model(
            src_vocab_size=src_vocab_size,
            tgt_vocab_size=tgt_vocab_size,
            model_size=model_size,
            pad_idx=pad_idx
        )
        model = model.to(device)
        
        # In th√¥ng tin model
        print_model_info(model, model_size)
        
        # T·∫°o dummy data
        src = torch.randint(1, src_vocab_size, (batch_size, src_len)).to(device)
        tgt = torch.randint(1, tgt_vocab_size, (batch_size, tgt_len)).to(device)
        
        # Forward pass
        print(f"\nForward pass:")
        print(f"  Source shape: {src.shape}")
        print(f"  Target shape: {tgt.shape}")
        
        with torch.no_grad():
            output = model(src, tgt)
        
        print(f"  Output shape: {output.shape}")
        print(f"  Expected: [{batch_size}, {tgt_len}, {tgt_vocab_size}]")
        print(f"  ‚úì Shape correct!")
        
        # Test encode v√† decode ri√™ng
        print(f"\nTest encode & decode separately:")
        with torch.no_grad():
            encoder_output, src_mask = model.encode(src)
            decoder_output = model.decode(tgt, encoder_output, src_mask)
        
        print(f"  Encoder output shape: {encoder_output.shape}")
        print(f"  Decoder output shape: {decoder_output.shape}")
        print(f"  ‚úì Encode/Decode work correctly!")
        
        # Ki·ªÉm tra output gi·ªëng nhau
        print(f"\nVerify output consistency:")
        with torch.no_grad():
            output_combined = model(src, tgt)
        
        is_same = torch.allclose(output_combined, decoder_output, atol=1e-6)
        print(f"  Forward == Encode+Decode: {is_same}")
        print(f"  ‚úì Model is consistent!")
    
    print("\n" + "="*70)
    print("‚úì T·∫§T C·∫¢ TESTS PASSED!")
    print("="*70)
    
    print("\nüìù G·ª¢I √ù S·ª¨ D·ª§NG:")
    print("  - D√πng 'tiny' ƒë·ªÉ debug v√† test nhanh")
    print("  - D√πng 'small' ƒë·ªÉ train tr√™n CPU ho·∫∑c GPU nh·ªè")
    print("  - D√πng 'base' ƒë·ªÉ c√≥ k·∫øt qu·∫£ t·ªët (c·∫ßn GPU)")
    print("  - D√πng 'large' ch·ªâ khi c√≥ GPU m·∫°nh")