# Transformer Machine Translation: Vietnamese â†’ English

Dá»± Ã¡n xÃ¢y dá»±ng mÃ´ hÃ¬nh Transformer tá»« Ä‘áº§u (from scratch) cho bÃ i toÃ¡n dá»‹ch mÃ¡y Tiáº¿ng Viá»‡t - Tiáº¿ng Anh.

## ğŸ“‹ Má»¥c lá»¥c

- [Tá»•ng quan](#tá»•ng-quan)
- [CÃ i Ä‘áº·t](#cÃ i-Ä‘áº·t)
- [Cáº¥u trÃºc dá»± Ã¡n](#cáº¥u-trÃºc-dá»±-Ã¡n)
- [HÆ°á»›ng dáº«n sá»­ dá»¥ng](#hÆ°á»›ng-dáº«n-sá»­-dá»¥ng)
- [Kiáº¿n trÃºc](#kiáº¿n-trÃºc)
- [Káº¿t quáº£](#káº¿t-quáº£)
- [Cáº£i tiáº¿n](#cáº£i-tiáº¿n)

---

## ğŸ¯ Tá»•ng quan

### Má»¥c tiÃªu
XÃ¢y dá»±ng hoÃ n chá»‰nh mÃ´ hÃ¬nh Transformer tá»« cÃ¡c thÃ nh pháº§n cÆ¡ báº£n Ä‘á»ƒ thá»±c hiá»‡n dá»‹ch mÃ¡y Viâ†’En, Ä‘áº¡t Ä‘iá»ƒm cao nháº¥t.

### Dataset
- **IWSLT 2015 Vietnamese-English**
- Train: ~133K cáº·p cÃ¢u
- Validation: ~1.5K cáº·p cÃ¢u  
- Test: ~1.3K cáº·p cÃ¢u

### Highlights
âœ… **100% code from scratch** - Táº¥t cáº£ components Ä‘Æ°á»£c implement tay  
âœ… **Label Smoothing** - Giáº£m overfitting  
âœ… **Warmup Learning Rate Scheduler** - á»”n Ä‘á»‹nh training  
âœ… **Beam Search Decoding** - Cáº£i thiá»‡n cháº¥t lÆ°á»£ng dá»‹ch  
âœ… **Bucket Sampling** - Tá»‘i Æ°u tá»‘c Ä‘á»™ training  
âœ… **Dynamic Padding** - Giáº£m computation overhead  

---

## ğŸš€ CÃ i Ä‘áº·t

### YÃªu cáº§u há»‡ thá»‘ng
- Python 3.8+
- CUDA 11.0+ (náº¿u dÃ¹ng GPU)
- RAM: 8GB+ 
- Disk: 5GB+

### CÃ i Ä‘áº·t thÆ° viá»‡n

```bash
# Clone repository
git clone <your-repo>
cd transformer-vi-en

# Táº¡o virtual environment (khuyÃªn dÃ¹ng)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# hoáº·c
venv\Scripts\activate  # Windows

# CÃ i Ä‘áº·t dependencies
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install datasets tokenizers sentencepiece tqdm matplotlib
```

### Kiá»ƒm tra cÃ i Ä‘áº·t

```python
import torch
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"CUDA version: {torch.version.cuda}")
```

---

## ğŸ“ Cáº¥u trÃºc dá»± Ã¡n

```
transformer-vi-en/
â”œâ”€â”€ data_preprocessing.py          # Xá»­ lÃ½ dá»¯ liá»‡u IWSLT
â”œâ”€â”€ dataloader_module.py           # DataLoader vá»›i bucket sampling
â”œâ”€â”€ transformer_components.py      # Attention, FFN, Position Encoding
â”œâ”€â”€ transformer_encoder_decoder.py # Encoder & Decoder layers
â”œâ”€â”€ complete_transformer.py        # MÃ´ hÃ¬nh Transformer hoÃ n chá»‰nh
â”œâ”€â”€ training_module.py             # Training loop, Loss, Optimizer
â”œâ”€â”€ inference_evaluation.py        # Decoding, BLEU score
â”œâ”€â”€ main_pipeline.py               # Script chÃ­nh
â”œâ”€â”€ README.md                      # HÆ°á»›ng dáº«n nÃ y
â”œâ”€â”€ checkpoints/                   # LÆ°u model checkpoints
â”œâ”€â”€ results/                       # LÆ°u káº¿t quáº£, translations
â””â”€â”€ logs/                          # Training logs
```

---

## ğŸ’» HÆ°á»›ng dáº«n sá»­ dá»¥ng

### Quick Start - Cháº¡y toÃ n bá»™ pipeline

```bash
# Cháº¡y táº¥t cáº£: Data â†’ Train â†’ Evaluate
python main_pipeline.py --stage all --model_size base --epochs 20 --batch_size 32
```

### Cháº¡y tá»«ng bÆ°á»›c

#### BÆ°á»›c 1: Chuáº©n bá»‹ dá»¯ liá»‡u

```bash
python main_pipeline.py --stage 1
```

Táº¡o ra:
- `cleaned_data.json` - Dá»¯ liá»‡u Ä‘Ã£ lÃ m sáº¡ch
- `vi_vocab.pkl` - Vocabulary tiáº¿ng Viá»‡t
- `en_vocab.pkl` - Vocabulary tiáº¿ng Anh
- `processed_data.pkl` - Dá»¯ liá»‡u Ä‘Ã£ encode

#### BÆ°á»›c 2-3: Táº¡o model vÃ  DataLoaders

```bash
python main_pipeline.py --stage 3 --model_size base
```

**Model sizes:**
- `tiny`: 2 layers, 256 dim â†’ ~5M params (test nhanh)
- `small`: 4 layers, 256 dim â†’ ~10M params (laptop, CPU)
- `base`: 6 layers, 512 dim â†’ ~65M params (khuyÃªn dÃ¹ng, cáº§n GPU)
- `large`: 6 layers, 1024 dim â†’ ~260M params (GPU máº¡nh)

#### BÆ°á»›c 4: Training

```bash
python main_pipeline.py --stage 4 --model_size base --epochs 20 --batch_size 32
```

**Hyperparameters quan trá»ng:**
- `--epochs`: Sá»‘ epochs (khuyÃªn 15-20)
- `--batch_size`: Batch size (32 cho GPU 8GB, 64 cho GPU 16GB+)
- `--model_size`: KÃ­ch thÆ°á»›c model

**Monitoring training:**
- Loss & Perplexity hiá»ƒn thá»‹ real-time
- Checkpoints tá»± Ä‘á»™ng lÆ°u má»—i epoch
- Best model lÆ°u vÃ o `checkpoints/best_model.pt`

#### BÆ°á»›c 5: Evaluation

```bash
python main_pipeline.py --stage 5 --beam_size 5
```

**Outputs:**
- BLEU scores (Greedy vs Beam Search)
- Sample translations
- Full translations: `results/translations_beam.txt`
- Scores: `results/bleu_scores.json`

#### BÆ°á»›c 6: Interactive Translation

```bash
python main_pipeline.py --stage 6
```

Dá»‹ch cÃ¢u tÆ°Æ¡ng tÃ¡c:
```
Tiáº¿ng Viá»‡t: Xin chÃ o, tÃ´i lÃ  má»™t sinh viÃªn.
Tiáº¿ng Anh:  hello , i am a student .
```

---

## ğŸ—ï¸ Kiáº¿n trÃºc

### Transformer Components

#### 1. Scaled Dot-Product Attention
```python
Attention(Q, K, V) = softmax(QÂ·K^T / âˆšd_k) Â· V
```

#### 2. Multi-Head Attention
- Parallel attention vá»›i nhiá»u heads (8 heads)
- Má»—i head há»c representation khÃ¡c nhau
- Concat vÃ  project vá» d_model

#### 3. Positional Encoding
```python
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

#### 4. Encoder Layer
```
Input 
â†’ Multi-Head Self-Attention 
â†’ Add & Norm 
â†’ Feed-Forward 
â†’ Add & Norm 
â†’ Output
```

#### 5. Decoder Layer
```
Input
â†’ Masked Self-Attention
â†’ Add & Norm
â†’ Cross-Attention (vá»›i Encoder)
â†’ Add & Norm
â†’ Feed-Forward
â†’ Add & Norm
â†’ Output
```

### Training Details

**Loss Function:**
- Label Smoothing Cross-Entropy
- Smoothing = 0.1 (giáº£m overconfidence)

**Optimizer:**
- Adam: Î²1=0.9, Î²2=0.98, Îµ=1e-9

**Learning Rate Schedule:**
```python
lr = d_model^(-0.5) * min(step^(-0.5), step * warmup^(-1.5))
```
- Warmup: 4000 steps
- TÄƒng dáº§n rá»“i giáº£m dáº§n

**Regularization:**
- Dropout: 0.1
- Gradient Clipping: max_norm=1.0
- Label Smoothing: 0.1

### Decoding Strategies

#### Greedy Search
- Chá»n token cÃ³ xÃ¡c suáº¥t cao nháº¥t
- Nhanh nhÆ°ng khÃ´ng tá»‘i Æ°u

#### Beam Search
- Giá»¯ top-K candidates
- Beam size = 5
- Length penalty: Î± = 0.6
- Cháº¥t lÆ°á»£ng tá»‘t hÆ¡n ~2-3 BLEU

---

## ğŸ“Š Káº¿t quáº£

### Expected Results (Base model, 20 epochs)

| Metric | Greedy Search | Beam Search (k=5) |
|--------|--------------|-------------------|
| BLEU Score | ~25-28 | ~27-30 |
| Inference Speed | Fast | Medium |

### Sample Translations

**VÃ­ dá»¥ 1:**
```
VI: tÃ´i Ä‘ang há»c vá» trÃ­ tuá»‡ nhÃ¢n táº¡o .
EN: i am learning about artificial intelligence .
```

**VÃ­ dá»¥ 2:**
```
VI: hÃ´m nay thá»i tiáº¿t ráº¥t Ä‘áº¹p .
EN: the weather is very nice today .
```

### Training Curves

Sau training, kiá»ƒm tra:
- `results/training_history.png` - Äá»“ thá»‹ Loss, Perplexity, LR
- Training loss giáº£m á»•n Ä‘á»‹nh
- Validation loss khÃ´ng tÄƒng (khÃ´ng overfit)

---

## ğŸš€ Cáº£i tiáº¿n Ä‘á»ƒ tÄƒng Ä‘iá»ƒm

### 1. Data Augmentation (KhuyÃªn dÃ¹ng â­)

**Back-translation:**
```python
# Dá»‹ch ENâ†’VI rá»“i dÃ¹ng lÃ m training data
# TÄƒng ~2-3 BLEU
```

**ThÃªm dá»¯ liá»‡u:**
```python
# ThÃªm TED Talks, OpenSubtitles
# TÄƒng vocabulary coverage
```

### 2. Model Improvements

**Relative Positional Encoding:**
- Thay Sinusoidal báº±ng Relative
- Tá»‘t hÆ¡n cho cÃ¢u dÃ i

**Layer Normalization Position:**
```python
# Pre-LN thay vÃ¬ Post-LN
# á»”n Ä‘á»‹nh hÆ¡n, train sÃ¢u hÆ¡n Ä‘Æ°á»£c
x = x + sublayer(norm(x))  # Pre-LN
```

**Tied Embeddings:**
```python
# Share weights giá»¯a encoder embedding vÃ  decoder embedding
# Giáº£m parameters ~10%
```

### 3. Training Tricks (Dá»… implement â­)

**Gradient Accumulation:**
```python
# TÄƒng effective batch size
# KhÃ´ng cáº§n GPU lá»›n
accumulation_steps = 4
```

**Mixed Precision Training:**
```python
# DÃ¹ng float16 thay float32
# Nhanh gáº¥p 2x, Ã­t VRAM hÆ¡n
from torch.cuda.amp import autocast, GradScaler
```

**Longer Training:**
```python
# Train 30-40 epochs thay vÃ¬ 20
# TÄƒng ~1-2 BLEU
```

### 4. Hyperparameter Tuning

**TÄƒng model size:**
```python
# Base â†’ Large
# +5-8 BLEU nhÆ°ng cáº§n GPU máº¡nh
```

**TÄƒng beam size:**
```python
# beam_size = 10
# +0.5-1 BLEU
```

**Label smoothing:**
```python
# Thá»­ 0.05, 0.1, 0.2
# TÃ¬m optimal value
```

### 5. Ensemble (TÄƒng nhiá»u nháº¥t â­â­â­)

```python
# Train 3-5 models vá»›i random seeds khÃ¡c nhau
# Average predictions
# +3-5 BLEU
```

### 6. Post-processing

**Moses Detokenizer:**
- Cáº£i thiá»‡n quality
- Xá»­ lÃ½ dáº¥u cÃ¢u Ä‘Ãºng hÆ¡n

**Unknown word replacement:**
- Copy tá»« source sang target náº¿u UNK

---

## ğŸ› Troubleshooting

### Out of Memory

```python
# Giáº£m batch_size
--batch_size 16

# Hoáº·c dÃ¹ng gradient accumulation
accumulation_steps = 2
```

### Training quÃ¡ cháº­m

```python
# DÃ¹ng model nhá» hÆ¡n
--model_size small

# TÄƒng num_workers
num_workers = 4

# DÃ¹ng bucket sampling (Ä‘Ã£ cÃ³)
```

### BLEU score tháº¥p

```python
# Train lÃ¢u hÆ¡n
--epochs 30

# TÄƒng model size
--model_size large

# DÃ¹ng beam search vá»›i beam_size lá»›n
--beam_size 10

# Data augmentation (back-translation)
```

### Overfitting

```python
# TÄƒng dropout
dropout = 0.2

# TÄƒng label smoothing
label_smoothing = 0.2

# ThÃªm dá»¯ liá»‡u
```

---

## ğŸ“š References

1. **Attention Is All You Need** (Vaswani et al., 2017)
   - Original Transformer paper

2. **The Annotated Transformer** (Harvard NLP)
   - http://nlp.seas.harvard.edu/annotated-transformer/

3. **Hugging Face Transformers**
   - https://huggingface.co/docs/transformers/

---

## ğŸ“ BÃ¡o cÃ¡o

### Ná»™i dung bÃ¡o cÃ¡o cáº§n cÃ³:

#### 1. Xá»­ lÃ½ dá»¯ liá»‡u
- Thá»‘ng kÃª dataset (sá»‘ cÃ¢u, Ä‘á»™ dÃ i trung bÃ¬nh)
- Quy trÃ¬nh lÃ m sáº¡ch
- Vocabulary size
- VÃ­ dá»¥ data sau preprocessing

#### 2. Kiáº¿n trÃºc
- SÆ¡ Ä‘á»“ kiáº¿n trÃºc Transformer
- Chi tiáº¿t tá»«ng component (Attention, FFN, etc.)
- Sá»‘ lÆ°á»£ng parameters
- Hyperparameters Ä‘Ã£ chá»n

#### 3. Training
- Äá»“ thá»‹ Loss/Perplexity (train & val)
- Äá»“ thá»‹ Learning Rate
- Training time
- Hardware specs

#### 4. Káº¿t quáº£
- **BLEU scores** (Greedy vs Beam)
- Sample translations (10-20 vÃ­ dá»¥ tá»‘t)
- Analysis: Loáº¡i cÃ¢u dá»‹ch tá»‘t/kÃ©m

#### 5. So sÃ¡nh cáº£i tiáº¿n
- Baseline (no improvements)
- Vá»›i improvements (label smoothing, beam search, etc.)
- Báº£ng so sÃ¡nh BLEU scores
- Ablation study náº¿u cÃ³

#### 6. Gemini Score (náº¿u yÃªu cáº§u)
- DÃ¹ng Gemini API Ä‘á»ƒ score translations
- So sÃ¡nh vá»›i BLEU

---

## ğŸ“ License

MIT License - Tá»± do sá»­ dá»¥ng cho há»c táº­p

---

## ğŸ‘¨â€ğŸ’» Author

Dá»± Ã¡n cho BTL NLP - Transformer Machine Translation

---

## ğŸ™ Acknowledgments

- IWSLT dataset
- PyTorch team
- Hugging Face Datasets

---

**Good luck vá»›i BTL! ğŸš€**

Náº¿u cÃ³ váº¥n Ä‘á», má»Ÿ issue hoáº·c liÃªn há»‡ qua email.