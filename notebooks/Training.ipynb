{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787cd86d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T04:43:18.963976Z",
     "iopub.status.busy": "2025-12-12T04:43:18.963199Z",
     "iopub.status.idle": "2025-12-12T04:43:22.620231Z",
     "shell.execute_reply": "2025-12-12T04:43:22.619246Z"
    },
    "executionInfo": {
     "elapsed": 2727,
     "status": "ok",
     "timestamp": 1764290628632,
     "user": {
      "displayName": "ƒê√† ƒê√†o",
      "userId": "06918234069347815596"
     },
     "user_tz": -420
    },
    "id": "iiiLpG-j54-w",
    "outputId": "010dc8f9-e049-4126-e4c5-87493259f8bb",
    "papermill": {
     "duration": 3.665285,
     "end_time": "2025-12-12T04:43:22.621621",
     "exception": false,
     "start_time": "2025-12-12T04:43:18.956336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CELL 1: Environment Setup & GPU Check\n",
    "print(\"=\"*70)\n",
    "print(\"KAGGLE TRANSFORMER NMT PIPELINE (SENTENCEPIECE)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# Kaggle paths\n",
    "CODE_PATH = '/kaggle/input/nlp-py-v2'  # Your code dataset\n",
    "DATA_PATH = '/kaggle/input/nlp-py-v2'  # Data files\n",
    "OUTPUT_PATH = '/kaggle/working'\n",
    "\n",
    "# Add code to Python path\n",
    "sys.path.insert(0, CODE_PATH)\n",
    "\n",
    "# Check GPU\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"  GPU INFORMATION\")\n",
    "print(\"=\"*70)\n",
    "if torch.cuda.is_available():\n",
    "    print(f\" GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\" Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\" CUDA version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"  WARNING: No GPU detected! Training will be slow.\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(f'{OUTPUT_PATH}/checkpoints', exist_ok=True)\n",
    "os.makedirs(f'{OUTPUT_PATH}/results', exist_ok=True)\n",
    "print(\"\\n Output directories created in /kaggle/working/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a0bed9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T04:43:22.631740Z",
     "iopub.status.busy": "2025-12-12T04:43:22.631433Z",
     "iopub.status.idle": "2025-12-12T04:43:22.635505Z",
     "shell.execute_reply": "2025-12-12T04:43:22.634927Z"
    },
    "papermill": {
     "duration": 0.010143,
     "end_time": "2025-12-12T04:43:22.636579",
     "exception": false,
     "start_time": "2025-12-12T04:43:22.626436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.listdir(\"/kaggle/input\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5925120",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T04:43:22.646217Z",
     "iopub.status.busy": "2025-12-12T04:43:22.646000Z",
     "iopub.status.idle": "2025-12-12T04:43:26.537471Z",
     "shell.execute_reply": "2025-12-12T04:43:26.536494Z"
    },
    "executionInfo": {
     "elapsed": 8794,
     "status": "ok",
     "timestamp": 1764290364678,
     "user": {
      "displayName": "ƒê√† ƒê√†o",
      "userId": "06918234069347815596"
     },
     "user_tz": -420
    },
    "id": "z5mUwrE-7cyX",
    "outputId": "7507f131-5b41-4e7b-cf48-d3d4d2827c01",
    "papermill": {
     "duration": 3.897802,
     "end_time": "2025-12-12T04:43:26.538810",
     "exception": false,
     "start_time": "2025-12-12T04:43:22.641008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CELL 2: Install SentencePiece\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INSTALLING SENTENCEPIECE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Install sentencepiece\n",
    "import subprocess\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"sentencepiece\"])\n",
    "\n",
    "print(\" SentencePiece installed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2c8f45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T04:43:26.549854Z",
     "iopub.status.busy": "2025-12-12T04:43:26.549390Z",
     "iopub.status.idle": "2025-12-12T04:43:26.568300Z",
     "shell.execute_reply": "2025-12-12T04:43:26.567478Z"
    },
    "executionInfo": {
     "elapsed": 62,
     "status": "ok",
     "timestamp": 1764290364742,
     "user": {
      "displayName": "ƒê√† ƒê√†o",
      "userId": "06918234069347815596"
     },
     "user_tz": -420
    },
    "id": "Txjk9pGs7eEu",
    "papermill": {
     "duration": 0.02537,
     "end_time": "2025-12-12T04:43:26.569381",
     "exception": false,
     "start_time": "2025-12-12T04:43:26.544011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CELL 3: Verify Files\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VERIFYING FILES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check for SentencePiece model files\n",
    "required_sp_files = [\n",
    "    'vi_sp.model',\n",
    "    'en_sp.model',\n",
    "    'vi_sp.vocab',\n",
    "    'en_sp.vocab'\n",
    "]\n",
    "\n",
    "# Check Python files\n",
    "required_files = [\n",
    "    'transformer_components.py',\n",
    "    'transformer_encoder_decoder.py',\n",
    "    'complete_transformer.py',\n",
    "    'training_module.py',\n",
    "    'inference_evaluation_v2.py',  # Updated inference file\n",
    "    'tokenizer_sentencepiece.py',  # New tokenizer file\n",
    "]\n",
    "\n",
    "print(\"\\n Python files:\")\n",
    "all_good = True\n",
    "for f in required_files:\n",
    "    file_path = f'{CODE_PATH}/{f}'\n",
    "    exists = os.path.exists(file_path)\n",
    "    status = \"True\" if exists else \"False\"\n",
    "    print(f\"  {status} {f}\")\n",
    "    if not exists:\n",
    "        all_good = False\n",
    "        if f in ['tokenizer_sentencepiece.py', 'inference_evaluation_v2.py']:\n",
    "            print(f\"        Missing new file - you need to add this to your Kaggle dataset\")\n",
    "\n",
    "# Check SentencePiece model files\n",
    "print(f\"\\nSentencePiece models:\")\n",
    "sp_files_exist = True\n",
    "for sp_file in required_sp_files:\n",
    "    file_path = f'{DATA_PATH}/{sp_file}'\n",
    "    exists = os.path.exists(file_path)\n",
    "    status = \"True\" if exists else \"False\"\n",
    "    if exists:\n",
    "        file_size = os.path.getsize(file_path) / (1024*1024)\n",
    "        print(f\"  {status} {sp_file} ({file_size:.2f} MB)\")\n",
    "    else:\n",
    "        print(f\"  {status} {sp_file}\")\n",
    "        sp_files_exist = False\n",
    "\n",
    "if not sp_files_exist:\n",
    "    print(\"\\n  SentencePiece models not found!\")\n",
    "    print(\"   You need to:\")\n",
    "    print(\"   1. Run tokenizer_sentencepiece.py locally to create .model files\")\n",
    "    print(\"   2. Upload vi_sp.model and en_sp.model to your Kaggle dataset\")\n",
    "\n",
    "# Check data files\n",
    "print(f\"\\n Data files:\")\n",
    "data_files = ['processed_data.pkl']\n",
    "for data_file in data_files:\n",
    "    file_path = f'{DATA_PATH}/{data_file}'\n",
    "    exists = os.path.exists(file_path)\n",
    "    status = \"True\" if exists else \"False\"\n",
    "    if exists:\n",
    "        file_size = os.path.getsize(file_path) / (1024*1024)\n",
    "        print(f\"  {status} {data_file} ({file_size:.2f} MB)\")\n",
    "    else:\n",
    "        print(f\"  {status} {data_file}\")\n",
    "\n",
    "if sp_files_exist:\n",
    "    print(\"\\n ALL SENTENCEPIECE FILES READY!\")\n",
    "else:\n",
    "    print(\"\\n MISSING SENTENCEPIECE FILES!\")\n",
    "    print(\"\\n TO CREATE SENTENCEPIECE MODELS:\")\n",
    "    print(\"   Run this command locally with your CSV:\")\n",
    "    print(\"   python tokenizer_sentencepiece.py --csv_path data.csv --vi_col src --en_col tgt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16be544d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T04:43:26.579722Z",
     "iopub.status.busy": "2025-12-12T04:43:26.579507Z",
     "iopub.status.idle": "2025-12-12T04:43:26.585251Z",
     "shell.execute_reply": "2025-12-12T04:43:26.584413Z"
    },
    "executionInfo": {
     "elapsed": 344,
     "status": "ok",
     "timestamp": 1764290365473,
     "user": {
      "displayName": "ƒê√† ƒê√†o",
      "userId": "06918234069347815596"
     },
     "user_tz": -420
    },
    "id": "2LNyi9w9AobA",
    "outputId": "676962a4-7045-4a7d-9a93-99b2ab3238aa",
    "papermill": {
     "duration": 0.012225,
     "end_time": "2025-12-12T04:43:26.586296",
     "exception": false,
     "start_time": "2025-12-12T04:43:26.574071",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CELL 4: Configuration\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚öôÔ∏è  CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "CONFIG = {\n",
    "    # Model settings\n",
    "    'model_size': 'tiny',              # tiny/small/base \n",
    "    # Training settings\n",
    "    'num_epochs': 5,\n",
    "    'batch_size': 32,                  # Increased due to smaller vocab\n",
    "    'warmup_steps': 2000,              # Reduced for smaller vocab\n",
    "    'label_smoothing': 0.1,\n",
    "    'save_every_batches': 89000,        # Save more frequently\n",
    "    'use_amp': True,                   # Mixed precision training\n",
    "    \n",
    "    # Evaluation settings\n",
    "    'beam_size': 5,\n",
    "}\n",
    "\n",
    "print(\"\\n Training Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key:20s}: {value}\")\n",
    "\n",
    "print(\"\\n Key Improvements with SentencePiece:\")\n",
    "print(\"   Vocab size: ~64K (was 426K) - 85% reduction!\")\n",
    "print(\"   No UNK tokens - handles all new words\")\n",
    "print(\"   Faster training - smaller vocab\")\n",
    "print(\"   Better for Vietnamese - handles accents well\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9d33d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T04:43:26.596260Z",
     "iopub.status.busy": "2025-12-12T04:43:26.595993Z",
     "iopub.status.idle": "2025-12-12T04:43:26.726655Z",
     "shell.execute_reply": "2025-12-12T04:43:26.725534Z"
    },
    "papermill": {
     "duration": 0.137131,
     "end_time": "2025-12-12T04:43:26.727923",
     "exception": false,
     "start_time": "2025-12-12T04:43:26.590792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CELL 5: Load Tokenizers\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" LOADING SENTENCEPIECE TOKENIZERS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from tokenizer_sentencepiece import SentencePieceTokenizer\n",
    "\n",
    "# Load tokenizers\n",
    "print(\"\\n Loading vocabularies...\")\n",
    "vi_tokenizer = SentencePieceTokenizer(f'{DATA_PATH}/vi_sp.model')\n",
    "en_tokenizer = SentencePieceTokenizer(f'{DATA_PATH}/en_sp.model')\n",
    "\n",
    "print(f\"\\n Tokenizers loaded successfully!\")\n",
    "print(f\"   Vietnamese vocab: {len(vi_tokenizer):,} tokens\")\n",
    "print(f\"   English vocab: {len(en_tokenizer):,} tokens\")\n",
    "print(f\"   Total vocab: {len(vi_tokenizer) + len(en_tokenizer):,} tokens\")\n",
    "\n",
    "# Test tokenizers\n",
    "print(f\"\\n Testing tokenizers:\")\n",
    "test_vi = \"xin ch√†o, t√¥i l√† sinh vi√™n\"\n",
    "test_en = \"hello, i am a student\"\n",
    "\n",
    "vi_ids = vi_tokenizer.encode(test_vi)\n",
    "en_ids = en_tokenizer.encode(test_en)\n",
    "\n",
    "print(f\"\\n  VI: '{test_vi}'\")\n",
    "print(f\"      Tokens: {vi_tokenizer.encode_as_pieces(test_vi)}\")\n",
    "print(f\"      Decoded: '{vi_tokenizer.decode(vi_ids)}'\")\n",
    "\n",
    "print(f\"\\n  EN: '{test_en}'\")\n",
    "print(f\"      Tokens: {en_tokenizer.encode_as_pieces(test_en)}\")\n",
    "print(f\"      Decoded: '{en_tokenizer.decode(en_ids)}'\")\n",
    "\n",
    "# Test with COMPLETELY NEW WORD\n",
    "new_word = \"ChatGPT DeepMind\"\n",
    "print(f\"\\n   NEW WORD TEST: '{new_word}'\")\n",
    "pieces = en_tokenizer.encode_as_pieces(new_word)\n",
    "print(f\"      Pieces: {pieces}\")\n",
    "print(f\"       No <unk>! Broken into subwords\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4e19f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T04:43:26.738872Z",
     "iopub.status.busy": "2025-12-12T04:43:26.738644Z",
     "iopub.status.idle": "2025-12-12T04:43:27.860852Z",
     "shell.execute_reply": "2025-12-12T04:43:27.859880Z"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1764290365488,
     "user": {
      "displayName": "ƒê√† ƒê√†o",
      "userId": "06918234069347815596"
     },
     "user_tz": -420
    },
    "id": "y2x_pHYEAvQv",
    "outputId": "b87ef7e2-014b-4cff-9c4c-fcc7278ff511",
    "papermill": {
     "duration": 1.128829,
     "end_time": "2025-12-12T04:43:27.862037",
     "exception": false,
     "start_time": "2025-12-12T04:43:26.733208",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CELL 6: Create Models\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"  STAGE 2: CREATE MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from complete_transformer import create_model, print_model_info\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\n  Device: {device}\")\n",
    "\n",
    "# Create VI->EN model\n",
    "print(f\"\\nüî® Creating VI->EN {CONFIG['model_size']} model...\")\n",
    "model_vi_en, model_config = create_model(\n",
    "    src_vocab_size=len(vi_tokenizer),\n",
    "    tgt_vocab_size=len(en_tokenizer),\n",
    "    model_size=CONFIG['model_size'],\n",
    "    pad_idx=0\n",
    ")\n",
    "\n",
    "print_model_info(model_vi_en, CONFIG['model_size'])\n",
    "model_vi_en = model_vi_en.to(device)\n",
    "print(f\"\\n VI->EN Model moved to {device}\")\n",
    "\n",
    "# Create EN->VI model\n",
    "print(f\"\\n Creating EN->VI {CONFIG['model_size']} model...\")\n",
    "model_en_vi, model_config_en_vi = create_model(\n",
    "    src_vocab_size=len(en_tokenizer),\n",
    "    tgt_vocab_size=len(vi_tokenizer),\n",
    "    model_size=CONFIG['model_size'],\n",
    "    pad_idx=0\n",
    ")\n",
    "\n",
    "print_model_info(model_en_vi, CONFIG['model_size'])\n",
    "model_en_vi = model_en_vi.to(device)\n",
    "print(f\"\\n EN->VI Model moved to {device}\")\n",
    "\n",
    "print(f\"\\n Comparison with old vocab:\")\n",
    "old_total = 174608 + 251853\n",
    "new_total = len(vi_tokenizer) + len(en_tokenizer)\n",
    "reduction = (1 - new_total / old_total) * 100\n",
    "print(f\"   Old total vocab: {old_total:,}\")\n",
    "print(f\"   New total vocab: {new_total:,}\")\n",
    "print(f\"   Reduction: {reduction:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a702e97b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T04:43:27.874056Z",
     "iopub.status.busy": "2025-12-12T04:43:27.873829Z",
     "iopub.status.idle": "2025-12-12T04:43:37.092210Z",
     "shell.execute_reply": "2025-12-12T04:43:37.090825Z"
    },
    "papermill": {
     "duration": 9.225824,
     "end_time": "2025-12-12T04:43:37.093473",
     "exception": false,
     "start_time": "2025-12-12T04:43:27.867649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RESUME TRAINING FROM EPOCH 5 CHECKPOINT\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" RESUMING TRAINING FROM CHECKPOINT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# STEP 1: LOCATE YOUR CHECKPOINT FILES\n",
    "\n",
    "\n",
    "print(\"\\n Looking for checkpoints...\")\n",
    "\n",
    "# Path to your checkpoints (adjust if different)\n",
    "CHECKPOINT_DIR_VI_EN = '/kaggle/input/checkpoint-nlp'\n",
    "CHECKPOINT_DIR_EN_VI = '//kaggle/input/checkpoint-nlp'\n",
    "\n",
    "# Find all epoch checkpoints\n",
    "vi_en_checkpoints = glob.glob(f'{CHECKPOINT_DIR_VI_EN}/best_model_vi_en_14.pt')\n",
    "en_vi_checkpoints = glob.glob(f'{CHECKPOINT_DIR_EN_VI}/best_model_en_vi_14.pt')\n",
    "\n",
    "if vi_en_checkpoints:\n",
    "    print(f\"\\n Found {len(vi_en_checkpoints)} VI->EN checkpoints:\")\n",
    "    for cp in sorted(vi_en_checkpoints):\n",
    "        print(f\"   - {os.path.basename(cp)}\")\n",
    "else:\n",
    "    print(\"\\n No VI->EN checkpoints found!\")\n",
    "\n",
    "if en_vi_checkpoints:\n",
    "    print(f\"\\n Found {len(en_vi_checkpoints)} EN->VI checkpoints:\")\n",
    "    for cp in sorted(en_vi_checkpoints):\n",
    "        print(f\"   - {os.path.basename(cp)}\")\n",
    "else:\n",
    "    print(\"\\n No EN->VI checkpoints found!\")\n",
    "\n",
    "\n",
    "# STEP 2: SELECT CHECKPOINT TO RESUME FROM\n",
    "\n",
    "\n",
    "# Use specific epoch (RECOMMENDED for you - Epoch 5)\n",
    "RESUME_FROM_EPOCH = 5\n",
    "resume_checkpoint_vi_en = f'{CHECKPOINT_DIR_VI_EN}/best_model_vi_en_14.pt'\n",
    "resume_checkpoint_en_vi = f'{CHECKPOINT_DIR_EN_VI}/best_model_en_vi_14.pt'\n",
    "\n",
    "print(f\"\\n Selected checkpoints:\")\n",
    "print(f\"   VI->EN: {os.path.basename(resume_checkpoint_vi_en)}\")\n",
    "print(f\"   EN->VI: {os.path.basename(resume_checkpoint_en_vi)}\")\n",
    "\n",
    "# STEP 3: LOAD CHECKPOINTS\n",
    "\n",
    "from training_module import load_checkpoint\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üì• LOADING CHECKPOINTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create optimizers (will be replaced by checkpoint state)\n",
    "optimizer_vi_en = torch.optim.Adam(\n",
    "    model_vi_en.parameters(), lr=1.0, betas=(0.9, 0.98), eps=1e-9\n",
    ")\n",
    "optimizer_en_vi = torch.optim.Adam(\n",
    "    model_en_vi.parameters(), lr=1.0, betas=(0.9, 0.98), eps=1e-9\n",
    ")\n",
    "\n",
    "# Create schedulers (will be replaced by checkpoint state)\n",
    "from training_module import TransformerLRScheduler\n",
    "\n",
    "scheduler_vi_en = TransformerLRScheduler(\n",
    "    optimizer_vi_en, d_model=model_config['d_model'], warmup_steps=CONFIG['warmup_steps']\n",
    ")\n",
    "scheduler_en_vi = TransformerLRScheduler(\n",
    "    optimizer_en_vi, d_model=model_config_en_vi['d_model'], warmup_steps=CONFIG['warmup_steps']\n",
    ")\n",
    "\n",
    "# Create scalers\n",
    "scaler_vi_en = GradScaler() if CONFIG['use_amp'] else None\n",
    "scaler_en_vi = GradScaler() if CONFIG['use_amp'] else None\n",
    "\n",
    "# Load VI->EN checkpoint\n",
    "print(\"\\n Loading VI->EN checkpoint...\")\n",
    "if os.path.exists(resume_checkpoint_vi_en):\n",
    "    model_vi_en, optimizer_vi_en, scheduler_vi_en, start_epoch_vi_en, start_batch_vi_en, history_vi_en, scaler_vi_en = load_checkpoint(\n",
    "        model_vi_en,\n",
    "        resume_checkpoint_vi_en,\n",
    "        device,\n",
    "        optimizer_vi_en,\n",
    "        scheduler_vi_en,\n",
    "        scaler_vi_en\n",
    "    )\n",
    "    print(f\" Loaded VI->EN from Epoch {start_epoch_vi_en}\")\n",
    "else:\n",
    "    print(f\" Checkpoint not found: {resume_checkpoint_vi_en}\")\n",
    "    print(\"   Will train from scratch!\")\n",
    "    start_epoch_vi_en = 1\n",
    "    start_batch_vi_en = 0\n",
    "    history_vi_en = {\n",
    "        'train_loss': [], 'train_ppl': [],\n",
    "        'val_loss': [], 'val_ppl': [],\n",
    "        'lr': [], 'epoch_times': [],\n",
    "        'batch_checkpoints': []\n",
    "    }\n",
    "\n",
    "# Load EN->VI checkpoint\n",
    "print(\"\\n Loading EN->VI checkpoint...\")\n",
    "if os.path.exists(resume_checkpoint_en_vi):\n",
    "    model_en_vi, optimizer_en_vi, scheduler_en_vi, start_epoch_en_vi, start_batch_en_vi, history_en_vi, scaler_en_vi = load_checkpoint(\n",
    "        model_en_vi,\n",
    "        resume_checkpoint_en_vi,\n",
    "        device,\n",
    "        optimizer_en_vi,\n",
    "        scheduler_en_vi,\n",
    "        scaler_en_vi\n",
    "    )\n",
    "    print(f\" Loaded EN->VI from Epoch {start_epoch_en_vi}\")\n",
    "else:\n",
    "    print(f\" Checkpoint not found: {resume_checkpoint_en_vi}\")\n",
    "    print(\"   Will train from scratch!\")\n",
    "    start_epoch_en_vi = 1\n",
    "    start_batch_en_vi = 0\n",
    "    history_en_vi = {\n",
    "        'train_loss': [], 'train_ppl': [],\n",
    "        'val_loss': [], 'val_ppl': [],\n",
    "        'lr': [], 'epoch_times': [],\n",
    "        'batch_checkpoints': []\n",
    "    }\n",
    "\n",
    "# STEP 4: VERIFY CHECKPOINT INFO\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CHECKPOINT INFO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüáªüá≥‚Üíüá¨üáß VI->EN Model:\")\n",
    "print(f\"   Resuming from: Epoch {start_epoch_vi_en}\")\n",
    "print(f\"   Previous epochs: {len(history_vi_en.get('train_loss', []))}\")\n",
    "if history_vi_en.get('train_loss'):\n",
    "    print(f\"   Last train loss: {history_vi_en['train_loss'][-1]:.4f}\")\n",
    "    print(f\"   Last train PPL: {history_vi_en['train_ppl'][-1]:.2f}\")\n",
    "if history_vi_en.get('val_loss'):\n",
    "    print(f\"   Last val loss: {history_vi_en['val_loss'][-1]:.4f}\")\n",
    "    print(f\"   Last val PPL: {history_vi_en['val_ppl'][-1]:.2f}\")\n",
    "if history_vi_en.get('lr'):\n",
    "    print(f\"   Current LR: {history_vi_en['lr'][-1]:.8f}\")\n",
    "\n",
    "print(\"\\nüá¨üáß‚Üíüáªüá≥ EN->VI Model:\")\n",
    "print(f\"   Resuming from: Epoch {start_epoch_en_vi}\")\n",
    "print(f\"   Previous epochs: {len(history_en_vi.get('train_loss', []))}\")\n",
    "if history_en_vi.get('train_loss'):\n",
    "    print(f\"   Last train loss: {history_en_vi['train_loss'][-1]:.4f}\")\n",
    "    print(f\"   Last train PPL: {history_en_vi['train_ppl'][-1]:.2f}\")\n",
    "if history_en_vi.get('val_loss'):\n",
    "    print(f\"   Last val loss: {history_en_vi['val_loss'][-1]:.4f}\")\n",
    "    print(f\"   Last val PPL: {history_en_vi['val_ppl'][-1]:.2f}\")\n",
    "if history_en_vi.get('lr'):\n",
    "    print(f\"   Current LR: {history_en_vi['lr'][-1]:.8f}\")\n",
    "\n",
    "# STEP 5: UPDATE CONFIG FOR CONTINUING TRAINING\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"  UPDATED TRAINING CONFIG\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate actual starting epoch (take max in case they differ)\n",
    "start_epoch = max(start_epoch_vi_en, start_epoch_en_vi) + 1  # +1 to start next epoch\n",
    "\n",
    "# Update config with NEW hyperparameters\n",
    "CONFIG_RESUME = {\n",
    "    'model_size': CONFIG['model_size'],\n",
    "    'num_epochs': 16,  #  Train to epoch 15 (from epoch 5 ‚Üí 10 more epochs)\n",
    "    'batch_size': 64,  #  INCREASED from 32\n",
    "    'warmup_steps': 4000,  #  INCREASED from 2000\n",
    "    'label_smoothing': 0.05,  #  DECREASED from 0.1\n",
    "    'save_every_batches': 89000,  #  FIXED from 89000\n",
    "    'use_amp': True,\n",
    "    'beam_size': 5,\n",
    "    'start_epoch': start_epoch,  #  Resume from here\n",
    "}\n",
    "\n",
    "print(f\"\\n Resume Configuration:\")\n",
    "for key, value in CONFIG_RESUME.items():\n",
    "    print(f\"   {key:20s}: {value}\")\n",
    "\n",
    "print(f\"\\n Changes from original:\")\n",
    "print(f\"    batch_size: 32 ‚Üí 64 (2x larger, more stable gradients)\")\n",
    "print(f\"    warmup_steps: 2000 ‚Üí 4000 (2x longer warmup)\")\n",
    "print(f\"    label_smoothing: 0.1 ‚Üí 0.05 (easier to learn)\")\n",
    "print(f\"    save_every_batches: 89000 ‚Üí 500 (safer)\")\n",
    "print(f\"    num_epochs: 5 ‚Üí 15 (train 10 more epochs)\")\n",
    "\n",
    "print(f\"\\n Expected improvement:\")\n",
    "print(f\"   Current PPL: ~22 (plateau)\")\n",
    "print(f\"   After 10 more epochs: ~16-18 (much better!)\")\n",
    "\n",
    "# STEP 6: IMPORTANT - ADJUST LEARNING RATE (OPTIONAL)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" LEARNING RATE ADJUSTMENT (OPTIONAL)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check current LR\n",
    "current_lr_vi_en = scheduler_vi_en.get_last_lr()[0]\n",
    "current_lr_en_vi = scheduler_en_vi.get_last_lr()[0]\n",
    "\n",
    "print(f\"\\nCurrent Learning Rates:\")\n",
    "print(f\"   VI->EN: {current_lr_vi_en:.8f}\")\n",
    "print(f\"   EN->VI: {current_lr_en_vi:.8f}\")\n",
    "\n",
    "# If LR is too low (< 1e-5), consider resetting it\n",
    "if current_lr_vi_en < 1e-5 or current_lr_en_vi < 1e-5:\n",
    "    print(\"\\n  WARNING: Learning rate is very low!\")\n",
    "    print(\"   This might be why you're plateauing.\")\n",
    "    print(\"\\n Options:\")\n",
    "    print(\"   1. Continue with current LR (conservative)\")\n",
    "    print(\"   2. Reset LR to higher value (aggressive)\")\n",
    "    print(\"   3. Use Cosine Annealing to restart LR (recommended)\")\n",
    "    \n",
    "    # Option: Manually increase LR\n",
    "    # RESET_LR = 5e-4  # Set higher LR\n",
    "    # for param_group in optimizer_vi_en.param_groups:\n",
    "    #     param_group['lr'] = RESET_LR\n",
    "    # for param_group in optimizer_en_vi.param_groups:\n",
    "    #     param_group['lr'] = RESET_LR\n",
    "    # print(f\"\\n Reset LR to {RESET_LR:.6f}\")\n",
    "else:\n",
    "    print(\"\\nLearning rate looks OK, continuing with current schedule\")\n",
    "\n",
    "\n",
    "# STEP 7: CREATE HISTORY DICT FOR TRAINING LOOP\n",
    "\n",
    "\n",
    "# Merge histories into the format expected by training loop\n",
    "history = {\n",
    "    'vi_en': history_vi_en if history_vi_en else {\n",
    "        'train_loss': [], 'train_ppl': [],\n",
    "        'val_loss': [], 'val_ppl': [],\n",
    "        'lr': [], 'epoch_times': [],\n",
    "        'batch_checkpoints': []\n",
    "    },\n",
    "    'en_vi': history_en_vi if history_en_vi else {\n",
    "        'train_loss': [], 'train_ppl': [],\n",
    "        'val_loss': [], 'val_ppl': [],\n",
    "        'lr': [], 'epoch_times': [],\n",
    "        'batch_checkpoints': []\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"READY TO RESUME TRAINING!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n Will start from Epoch {CONFIG_RESUME['start_epoch']}\")\n",
    "print(f\" Will train until Epoch {CONFIG_RESUME['num_epochs']}\")\n",
    "print(f\"  Estimated time: ~{(CONFIG_RESUME['num_epochs'] - CONFIG_RESUME['start_epoch'] + 1) * 15} minutes\")\n",
    "print(\"\\n Next: Run your training cell (Cell 8) with these loaded checkpoints!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Export variables for training loop\n",
    "CONFIG = CONFIG_RESUME  # Use updated config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce58fc1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T04:43:37.105504Z",
     "iopub.status.busy": "2025-12-12T04:43:37.105168Z",
     "iopub.status.idle": "2025-12-12T04:43:37.123484Z",
     "shell.execute_reply": "2025-12-12T04:43:37.121978Z"
    },
    "papermill": {
     "duration": 0.030442,
     "end_time": "2025-12-12T04:43:37.129501",
     "exception": false,
     "start_time": "2025-12-12T04:43:37.099059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# REDUCE DROPOUT + BOOST LR\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" ANTI-PLATEAU OPTIMIZATION STRATEGY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import torch\n",
    "\n",
    "# STEP 1: REDUCE DROPOUT (0.1 ‚Üí 0.05)\n",
    "print(\"\\n STEP 1: REDUCING DROPOUT\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def adjust_dropout(model, new_dropout, model_name=\"Model\"):\n",
    "    \"\"\"ƒêi·ªÅu ch·ªânh dropout c·ªßa model ƒëang training\"\"\"\n",
    "    count = 0\n",
    "    old_dropout = None\n",
    "    \n",
    "    for module in model.modules():\n",
    "        if isinstance(module, torch.nn.Dropout):\n",
    "            if old_dropout is None:\n",
    "                old_dropout = module.p\n",
    "            module.p = new_dropout\n",
    "            count += 1\n",
    "    \n",
    "    print(f\"   {model_name}:\")\n",
    "    print(f\"      ‚Ä¢ Updated {count} dropout layers\")\n",
    "    print(f\"      ‚Ä¢ Dropout: {old_dropout:.2f} ‚Üí {new_dropout:.2f}\")\n",
    "    \n",
    "    return count\n",
    "\n",
    "# Current dropout: 0.1\n",
    "# New dropout: 0.05 (gi·∫£m 50%)\n",
    "NEW_DROPOUT = 0.05\n",
    "\n",
    "print(f\"\\n Target Dropout: {NEW_DROPOUT}\")\n",
    "print(f\"   (Reduced from 0.1 to allow better learning)\")\n",
    "\n",
    "# Apply to both models\n",
    "count_vi_en = adjust_dropout(model_vi_en, NEW_DROPOUT, \"VI‚ÜíEN\")\n",
    "count_en_vi = adjust_dropout(model_en_vi, NEW_DROPOUT, \"EN‚ÜíVI\")\n",
    "\n",
    "print(f\"\\n Dropout reduction complete!\")\n",
    "print(f\"   This will reduce regularization and allow\")\n",
    "print(f\"   model to learn training data better.\")\n",
    "\n",
    "# STEP 2: BOOST LEARNING RATE (2.5x)\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\" STEP 2: BOOSTING LEARNING RATE\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Get current LR\n",
    "current_lr_vi_en = optimizer_vi_en.param_groups[0]['lr']\n",
    "current_lr_en_vi = optimizer_en_vi.param_groups[0]['lr']\n",
    "\n",
    "print(f\"\\n Current Learning Rates:\")\n",
    "print(f\"   VI‚ÜíEN: {current_lr_vi_en:.8f}\")\n",
    "print(f\"   EN‚ÜíVI: {current_lr_en_vi:.8f}\")\n",
    "print(f\"   (Too low! Model stuck in plateau)\")\n",
    "\n",
    "# Boost factor\n",
    "BOOST_FACTOR = 2.5  # TƒÉng 2.5x\n",
    "\n",
    "new_lr_vi_en = current_lr_vi_en * BOOST_FACTOR\n",
    "new_lr_en_vi = current_lr_en_vi * BOOST_FACTOR\n",
    "\n",
    "# Apply new LR to optimizer\n",
    "for param_group in optimizer_vi_en.param_groups:\n",
    "    param_group['lr'] = new_lr_vi_en\n",
    "\n",
    "for param_group in optimizer_en_vi.param_groups:\n",
    "    param_group['lr'] = new_lr_en_vi\n",
    "\n",
    "print(f\"\\n New Learning Rates (boosted {BOOST_FACTOR}x):\")\n",
    "print(f\"   VI‚ÜíEN: {new_lr_vi_en:.8f} ({current_lr_vi_en:.8f} √ó {BOOST_FACTOR})\")\n",
    "print(f\"   EN‚ÜíVI: {new_lr_en_vi:.8f} ({current_lr_en_vi:.8f} √ó {BOOST_FACTOR})\")\n",
    "\n",
    "print(f\"\\n LR boost complete!\")\n",
    "print(f\"   Higher LR will help escape plateau\")\n",
    "\n",
    "# STEP 3: OPTIONAL - SETUP COSINE ANNEALING WITH WARM RESTARTS\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\" STEP 3: COSINE ANNEALING SCHEDULER (OPTIONAL)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "USE_COSINE_SCHEDULER = False  #  SET False if you want manual LR only\n",
    "\n",
    "if USE_COSINE_SCHEDULER:\n",
    "    from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "    \n",
    "    # Create new schedulers\n",
    "    scheduler_vi_en = CosineAnnealingWarmRestarts(\n",
    "        optimizer_vi_en,\n",
    "        T_0=3,        # Restart LR every 3 epochs\n",
    "        T_mult=1,     # Keep restart period constant\n",
    "        eta_min=1e-6  # Minimum LR\n",
    "    )\n",
    "    \n",
    "    scheduler_en_vi = CosineAnnealingWarmRestarts(\n",
    "        optimizer_en_vi,\n",
    "        T_0=3,\n",
    "        T_mult=1,\n",
    "        eta_min=1e-6\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n Cosine Annealing enabled!\")\n",
    "    print(f\"   ‚Ä¢ Restart period: 3 epochs\")\n",
    "    print(f\"   ‚Ä¢ Min LR: 1e-6\")\n",
    "    print(f\"   ‚Ä¢ LR will cycle: high ‚Üí low ‚Üí restart\")\n",
    "    print(f\"\\n   This helps explore different learning rates\")\n",
    "    print(f\"   and escape local minima\")\n",
    "else:\n",
    "    print(f\"\\n  Keeping original Transformer LR scheduler\")\n",
    "    print(f\"   Using manual LR boost only\")\n",
    "\n",
    "# SUMMARY & EXPECTATIONS\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" OPTIMIZATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n Changes Applied:\")\n",
    "print(f\"    Dropout: 0.1 ‚Üí {NEW_DROPOUT} (-50%)\")\n",
    "print(f\"    LR VI‚ÜíEN: {current_lr_vi_en:.8f} ‚Üí {new_lr_vi_en:.8f} (+{(BOOST_FACTOR-1)*100:.0f}%)\")\n",
    "print(f\"    LR EN‚ÜíVI: {current_lr_en_vi:.8f} ‚Üí {new_lr_en_vi:.8f} (+{(BOOST_FACTOR-1)*100:.0f}%)\")\n",
    "if USE_COSINE_SCHEDULER:\n",
    "    print(f\"    Scheduler: Transformer ‚Üí Cosine Annealing\")\n",
    "\n",
    "# VERIFICATION\n",
    "print(\"\\n VERIFICATION:\")\n",
    "print(f\"   Dropout in VI‚ÜíEN: {next(m for m in model_vi_en.modules() if isinstance(m, torch.nn.Dropout)).p}\")\n",
    "print(f\"   Dropout in EN‚ÜíVI: {next(m for m in model_en_vi.modules() if isinstance(m, torch.nn.Dropout)).p}\")\n",
    "print(f\"   LR VI‚ÜíEN: {optimizer_vi_en.param_groups[0]['lr']:.8f}\")\n",
    "print(f\"   LR EN‚ÜíVI: {optimizer_en_vi.param_groups[0]['lr']:.8f}\")\n",
    "print(f\"   Scheduler: {'CosineAnnealingWarmRestarts' if USE_COSINE_SCHEDULER else 'TransformerLRScheduler'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a82cff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T04:43:37.141817Z",
     "iopub.status.busy": "2025-12-12T04:43:37.141240Z",
     "iopub.status.idle": "2025-12-12T04:44:05.748925Z",
     "shell.execute_reply": "2025-12-12T04:44:05.747975Z"
    },
    "papermill": {
     "duration": 28.614619,
     "end_time": "2025-12-12T04:44:05.750051",
     "exception": false,
     "start_time": "2025-12-12T04:43:37.135432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CELL 7: Load Data & Create DataLoaders\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" STAGE 3: CREATE DATALOADERS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import pickle\n",
    "from dataloader_module import create_dataloaders_with_bucketing\n",
    "\n",
    "# Load processed data\n",
    "print(\"\\n Loading processed data...\")\n",
    "with open(f'{DATA_PATH}/processed_data.pkl', 'rb') as f:\n",
    "    processed_data = pickle.load(f)\n",
    "\n",
    "# Create VI->EN dataloaders\n",
    "print(\"\\n Creating VI->EN dataloaders...\")\n",
    "train_loader_vi_en, val_loader_vi_en, test_loader_vi_en = create_dataloaders_with_bucketing(\n",
    "    processed_data,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "print(f\"\\n VI->EN DataLoaders ready!\")\n",
    "print(f\"   Train batches: {len(train_loader_vi_en)}\")\n",
    "print(f\"   Val batches: {len(val_loader_vi_en)}\")\n",
    "print(f\"   Test batches: {len(test_loader_vi_en)}\")\n",
    "\n",
    "# Create EN->VI dataloaders (reversed data)\n",
    "print(\"\\n Creating EN->VI dataloaders...\")\n",
    "reversed_data = {\n",
    "    'train': {\n",
    "        'src': processed_data['train']['tgt'],\n",
    "        'tgt': processed_data['train']['src']\n",
    "    },\n",
    "    'validation': {\n",
    "        'src': processed_data['validation']['tgt'],\n",
    "        'tgt': processed_data['validation']['src']\n",
    "    },\n",
    "    'test': {\n",
    "        'src': processed_data['test']['tgt'],\n",
    "        'tgt': processed_data['test']['src']\n",
    "    }\n",
    "}\n",
    "\n",
    "train_loader_en_vi, val_loader_en_vi, test_loader_en_vi = create_dataloaders_with_bucketing(\n",
    "    reversed_data,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "print(f\"\\n EN->VI DataLoaders ready!\")\n",
    "print(f\"   Train batches: {len(train_loader_en_vi)}\")\n",
    "print(f\"   Val batches: {len(val_loader_en_vi)}\")\n",
    "print(f\"   Test batches: {len(test_loader_en_vi)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260fd3a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T04:44:05.762821Z",
     "iopub.status.busy": "2025-12-12T04:44:05.762356Z",
     "iopub.status.idle": "2025-12-12T08:51:46.407636Z",
     "shell.execute_reply": "2025-12-12T08:51:46.406483Z"
    },
    "papermill": {
     "duration": 14860.653959,
     "end_time": "2025-12-12T08:51:46.409863",
     "exception": false,
     "start_time": "2025-12-12T04:44:05.755904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CELL 8: RESUME TRAINING WITH IMPROVED HYPERPARAMETERS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üöÄ RESUMING BIDIRECTIONAL TRAINING (IMPROVED)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from training_module import (\n",
    "    TransformerLRScheduler, \n",
    "    LabelSmoothingLoss,\n",
    "    calculate_perplexity,\n",
    "    save_checkpoint,\n",
    "    validate\n",
    ")\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "print(\"\\n  Setting up loss functions with NEW smoothing...\")\n",
    "criterion_vi_en = LabelSmoothingLoss(\n",
    "    vocab_size=len(en_tokenizer), \n",
    "    pad_idx=0, \n",
    "    smoothing=CONFIG['label_smoothing']  # Now 0.05 instead of 0.1\n",
    ")\n",
    "criterion_en_vi = LabelSmoothingLoss(\n",
    "    vocab_size=len(vi_tokenizer), \n",
    "    pad_idx=0, \n",
    "    smoothing=CONFIG['label_smoothing']\n",
    ")\n",
    "\n",
    "print(f\"Label smoothing: {CONFIG['label_smoothing']} (was 0.1)\")\n",
    "\n",
    "# RECREATE DATALOADERS WITH NEW BATCH SIZE\n",
    "\n",
    "if CONFIG['batch_size'] != 32:  # If we changed batch size\n",
    "    print(f\"\\n Recreating dataloaders with batch_size={CONFIG['batch_size']}...\")\n",
    "    \n",
    "    from dataloader_module import create_dataloaders_with_bucketing\n",
    "    \n",
    "    train_loader_vi_en, val_loader_vi_en, test_loader_vi_en = create_dataloaders_with_bucketing(\n",
    "        processed_data,\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    train_loader_en_vi, val_loader_en_vi, test_loader_en_vi = create_dataloaders_with_bucketing(\n",
    "        reversed_data,\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    print(f\" New batch size: {CONFIG['batch_size']} (was 32)\")\n",
    "    print(f\"   Batches per epoch: {len(train_loader_vi_en)}\")\n",
    "\n",
    "# TRAINING INFO\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" TRAINING CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n Resuming from: Epoch {CONFIG['start_epoch']}\")\n",
    "print(f\" Training until: Epoch {CONFIG['num_epochs']}\")\n",
    "print(f\" Epochs to train: {CONFIG['num_epochs'] - CONFIG['start_epoch'] + 1}\")\n",
    "print(f\"\\n Previous Performance:\")\n",
    "if history['vi_en']['val_ppl']:\n",
    "    print(f\"  VI‚ÜíEN PPL: {history['vi_en']['val_ppl'][-1]:.2f}\")\n",
    "if history['en_vi']['val_ppl']:\n",
    "    print(f\"  EN‚ÜíVI PPL: {history['en_vi']['val_ppl'][-1]:.2f}\")\n",
    "print(f\"\\n Expected after 10 more epochs: PPL ~16-18\")\n",
    "\n",
    "# Create checkpoint directories\n",
    "os.makedirs(f'{OUTPUT_PATH}/checkpoints/vi_en', exist_ok=True)\n",
    "os.makedirs(f'{OUTPUT_PATH}/checkpoints/en_vi', exist_ok=True)\n",
    "\n",
    "print(f\"\\n Training start: {time.strftime('%H:%M:%S')}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "best_val_loss_vi_en = min(history['vi_en']['val_loss']) if history['vi_en']['val_loss'] else float('inf')\n",
    "best_val_loss_en_vi = min(history['en_vi']['val_loss']) if history['en_vi']['val_loss'] else float('inf')\n",
    "\n",
    "# TRAINING LOOP - RESUME FROM start_epoch\n",
    "\n",
    "for epoch in range(CONFIG['start_epoch'], CONFIG['num_epochs'] + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EPOCH {epoch}/{CONFIG['num_epochs']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Train mode\n",
    "    model_vi_en.train()\n",
    "    model_en_vi.train()\n",
    "    \n",
    "    # Statistics\n",
    "    total_loss_vi_en = 0\n",
    "    total_tokens_vi_en = 0\n",
    "    total_loss_en_vi = 0\n",
    "    total_tokens_en_vi = 0\n",
    "    batch_loss_vi_en = 0\n",
    "    batch_tokens_vi_en = 0\n",
    "    batch_loss_en_vi = 0\n",
    "    batch_tokens_en_vi = 0\n",
    "    \n",
    "    # Iterators\n",
    "    iter_vi_en = iter(train_loader_vi_en)\n",
    "    iter_en_vi = iter(train_loader_en_vi)\n",
    "    max_batches = max(len(train_loader_vi_en), len(train_loader_en_vi))\n",
    "    \n",
    "    progress_bar = tqdm(range(max_batches), desc=f'Epoch {epoch}')\n",
    "    \n",
    "    for batch_idx in progress_bar:\n",
    "        #  TRAIN VI->EN \n",
    "        try:\n",
    "            src, tgt, _, _ = next(iter_vi_en)\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            \n",
    "            optimizer_vi_en.zero_grad()\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "            \n",
    "            if CONFIG['use_amp']:\n",
    "                with autocast():\n",
    "                    output = model_vi_en(src, tgt_input)\n",
    "                    loss_vi_en = criterion_vi_en(output, tgt_output)\n",
    "                scaler_vi_en.scale(loss_vi_en).backward()\n",
    "                scaler_vi_en.unscale_(optimizer_vi_en)\n",
    "                torch.nn.utils.clip_grad_norm_(model_vi_en.parameters(), 1.0)\n",
    "                scaler_vi_en.step(optimizer_vi_en)\n",
    "                scaler_vi_en.update()\n",
    "            else:\n",
    "                output = model_vi_en(src, tgt_input)\n",
    "                loss_vi_en = criterion_vi_en(output, tgt_output)\n",
    "                loss_vi_en.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model_vi_en.parameters(), 1.0)\n",
    "                optimizer_vi_en.step()\n",
    "            \n",
    "            scheduler_vi_en.step()\n",
    "            \n",
    "            num_tokens = (tgt_output != 0).sum().item()\n",
    "            total_loss_vi_en += loss_vi_en.item() * num_tokens\n",
    "            total_tokens_vi_en += num_tokens\n",
    "            batch_loss_vi_en += loss_vi_en.item() * num_tokens\n",
    "            batch_tokens_vi_en += num_tokens\n",
    "            \n",
    "            del src, tgt, output, loss_vi_en\n",
    "            \n",
    "        except StopIteration:\n",
    "            iter_vi_en = iter(train_loader_vi_en)\n",
    "        \n",
    "        #  TRAIN EN->VI \n",
    "        try:\n",
    "            src, tgt, _, _ = next(iter_en_vi)\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            \n",
    "            optimizer_en_vi.zero_grad()\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "            \n",
    "            if CONFIG['use_amp']:\n",
    "                with autocast():\n",
    "                    output = model_en_vi(src, tgt_input)\n",
    "                    loss_en_vi = criterion_en_vi(output, tgt_output)\n",
    "                scaler_en_vi.scale(loss_en_vi).backward()\n",
    "                scaler_en_vi.unscale_(optimizer_en_vi)\n",
    "                torch.nn.utils.clip_grad_norm_(model_en_vi.parameters(), 1.0)\n",
    "                scaler_en_vi.step(optimizer_en_vi)\n",
    "                scaler_en_vi.update()\n",
    "            else:\n",
    "                output = model_en_vi(src, tgt_input)\n",
    "                loss_en_vi = criterion_en_vi(output, tgt_output)\n",
    "                loss_en_vi.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model_en_vi.parameters(), 1.0)\n",
    "                optimizer_en_vi.step()\n",
    "            \n",
    "            scheduler_en_vi.step()\n",
    "            \n",
    "            num_tokens = (tgt_output != 0).sum().item()\n",
    "            total_loss_en_vi += loss_en_vi.item() * num_tokens\n",
    "            total_tokens_en_vi += num_tokens\n",
    "            batch_loss_en_vi += loss_en_vi.item() * num_tokens\n",
    "            batch_tokens_en_vi += num_tokens\n",
    "            \n",
    "            del src, tgt, output, loss_en_vi\n",
    "            \n",
    "        except StopIteration:\n",
    "            iter_en_vi = iter(train_loader_en_vi)\n",
    "        \n",
    "        # Update progress\n",
    "        current_loss_vi_en = total_loss_vi_en / max(total_tokens_vi_en, 1)\n",
    "        current_loss_en_vi = total_loss_en_vi / max(total_tokens_en_vi, 1)\n",
    "        current_ppl_vi_en = calculate_perplexity(current_loss_vi_en)\n",
    "        current_ppl_en_vi = calculate_perplexity(current_loss_en_vi)\n",
    "        current_lr = scheduler_vi_en.get_last_lr()[0]\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            'VI‚ÜíEN_loss': f'{current_loss_vi_en:.4f}',\n",
    "            'VI‚ÜíEN_ppl': f'{current_ppl_vi_en:.1f}',\n",
    "            'EN‚ÜíVI_loss': f'{current_loss_en_vi:.4f}',\n",
    "            'EN‚ÜíVI_ppl': f'{current_ppl_en_vi:.1f}',\n",
    "            'LR': f'{current_lr:.6f}'\n",
    "        })\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (batch_idx + 1) % CONFIG['save_every_batches'] == 0:\n",
    "            avg_loss_vi_en = batch_loss_vi_en / max(batch_tokens_vi_en, 1)\n",
    "            avg_loss_en_vi = batch_loss_en_vi / max(batch_tokens_en_vi, 1)\n",
    "            \n",
    "            print(f\"\\n Checkpoint at batch {batch_idx + 1}/{max_batches}\")\n",
    "            \n",
    "            save_checkpoint(\n",
    "                model_vi_en, optimizer_vi_en, scheduler_vi_en,\n",
    "                epoch, batch_idx + 1, avg_loss_vi_en, None,\n",
    "                f'{OUTPUT_PATH}/checkpoints/vi_en',\n",
    "                history=history['vi_en'], scaler=scaler_vi_en\n",
    "            )\n",
    "            \n",
    "            save_checkpoint(\n",
    "                model_en_vi, optimizer_en_vi, scheduler_en_vi,\n",
    "                epoch, batch_idx + 1, avg_loss_en_vi, None,\n",
    "                f'{OUTPUT_PATH}/checkpoints/en_vi',\n",
    "                history=history['en_vi'], scaler=scaler_en_vi\n",
    "            )\n",
    "            \n",
    "            batch_loss_vi_en = 0\n",
    "            batch_tokens_vi_en = 0\n",
    "            batch_loss_en_vi = 0\n",
    "            batch_tokens_en_vi = 0\n",
    "    \n",
    "    # VALIDATION \n",
    "    print(\"\\n Validating...\")\n",
    "    \n",
    "    val_loss_vi_en, val_ppl_vi_en = validate(\n",
    "        model_vi_en, val_loader_vi_en, criterion_vi_en, device\n",
    "    )\n",
    "    val_loss_en_vi, val_ppl_en_vi = validate(\n",
    "        model_en_vi, val_loader_en_vi, criterion_en_vi, device\n",
    "    )\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_loss_vi_en = total_loss_vi_en / total_tokens_vi_en\n",
    "    train_loss_en_vi = total_loss_en_vi / total_tokens_en_vi\n",
    "    train_ppl_vi_en = calculate_perplexity(train_loss_vi_en)\n",
    "    train_ppl_en_vi = calculate_perplexity(train_loss_en_vi)\n",
    "    \n",
    "    # Update history\n",
    "    history['vi_en']['train_loss'].append(train_loss_vi_en)\n",
    "    history['vi_en']['train_ppl'].append(train_ppl_vi_en)\n",
    "    history['vi_en']['val_loss'].append(val_loss_vi_en)\n",
    "    history['vi_en']['val_ppl'].append(val_ppl_vi_en)\n",
    "    history['vi_en']['lr'].append(scheduler_vi_en.get_last_lr()[0])\n",
    "    history['vi_en']['epoch_times'].append(epoch_time)\n",
    "    \n",
    "    history['en_vi']['train_loss'].append(train_loss_en_vi)\n",
    "    history['en_vi']['train_ppl'].append(train_ppl_en_vi)\n",
    "    history['en_vi']['val_loss'].append(val_loss_en_vi)\n",
    "    history['en_vi']['val_ppl'].append(val_ppl_en_vi)\n",
    "    history['en_vi']['lr'].append(scheduler_en_vi.get_last_lr()[0])\n",
    "    history['en_vi']['epoch_times'].append(epoch_time)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EPOCH {epoch}/{CONFIG['num_epochs']} COMPLETE - {epoch_time/60:.2f} min\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    print(f\"\\nüáªüá≥‚Üíüá¨üáß VI‚ÜíEN:\")\n",
    "    print(f\"  Train: Loss={train_loss_vi_en:.4f}, PPL={train_ppl_vi_en:.2f}\")\n",
    "    print(f\"  Val:   Loss={val_loss_vi_en:.4f}, PPL={val_ppl_vi_en:.2f}\")\n",
    "    print(f\"  LR: {scheduler_vi_en.get_last_lr()[0]:.6f}\")\n",
    "    \n",
    "    print(f\"\\nüá¨üáß‚Üíüáªüá≥ EN‚ÜíVI:\")\n",
    "    print(f\"  Train: Loss={train_loss_en_vi:.4f}, PPL={train_ppl_en_vi:.2f}\")\n",
    "    print(f\"  Val:   Loss={val_loss_en_vi:.4f}, PPL={val_ppl_en_vi:.2f}\")\n",
    "    print(f\"  LR: {scheduler_en_vi.get_last_lr()[0]:.6f}\")\n",
    "    \n",
    "    # Check best\n",
    "    is_best_vi_en = val_loss_vi_en < best_val_loss_vi_en\n",
    "    is_best_en_vi = val_loss_en_vi < best_val_loss_en_vi\n",
    "    \n",
    "    if is_best_vi_en:\n",
    "        best_val_loss_vi_en = val_loss_vi_en\n",
    "        print(f\"\\n   NEW BEST VI‚ÜíEN! Val PPL: {val_ppl_vi_en:.2f}\")\n",
    "    \n",
    "    if is_best_en_vi:\n",
    "        best_val_loss_en_vi = val_loss_en_vi\n",
    "        print(f\"   NEW BEST EN‚ÜíVI! Val PPL: {val_ppl_en_vi:.2f}\")\n",
    "    \n",
    "    # Save epoch checkpoint\n",
    "    print(f\"\\n Saving epoch checkpoint...\")\n",
    "    \n",
    "    save_checkpoint(\n",
    "        model_vi_en, optimizer_vi_en, scheduler_vi_en,\n",
    "        epoch, 0, train_loss_vi_en, val_loss_vi_en,\n",
    "        f'{OUTPUT_PATH}/checkpoints/vi_en',\n",
    "        history=history['vi_en'], scaler=scaler_vi_en, is_best=is_best_vi_en\n",
    "    )\n",
    "    \n",
    "    save_checkpoint(\n",
    "        model_en_vi, optimizer_en_vi, scheduler_en_vi,\n",
    "        epoch, 0, train_loss_en_vi, val_loss_en_vi,\n",
    "        f'{OUTPUT_PATH}/checkpoints/en_vi',\n",
    "        history=history['en_vi'], scaler=scaler_en_vi, is_best=is_best_en_vi\n",
    "    )\n",
    "\n",
    "print(f\"\\n Training end: {time.strftime('%H:%M:%S')}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n FINAL RESULTS:\")\n",
    "print(f\"\\nüáªüá≥‚Üíüá¨üáß VI‚ÜíEN:\")\n",
    "print(f\"  Best Val PPL: {calculate_perplexity(best_val_loss_vi_en):.2f}\")\n",
    "print(f\"  Final Val PPL: {history['vi_en']['val_ppl'][-1]:.2f}\")\n",
    "\n",
    "print(f\"\\nüá¨üáß‚Üíüáªüá≥ EN‚ÜíVI:\")\n",
    "print(f\"  Best Val PPL: {calculate_perplexity(best_val_loss_en_vi):.2f}\")\n",
    "print(f\"  Final Val PPL: {history['en_vi']['val_ppl'][-1]:.2f}\")\n",
    "\n",
    "print(f\"\\n Total training time: {sum(history['vi_en']['epoch_times'])/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae76dc1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T08:52:05.715352Z",
     "iopub.status.busy": "2025-12-12T08:52:05.715004Z",
     "iopub.status.idle": "2025-12-12T08:58:34.403995Z",
     "shell.execute_reply": "2025-12-12T08:58:34.403041Z"
    },
    "papermill": {
     "duration": 398.151103,
     "end_time": "2025-12-12T08:58:34.405483",
     "exception": false,
     "start_time": "2025-12-12T08:51:56.254380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CELL 9: COMPREHENSIVE EVALUATION - BLEU & METRICS \n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPREHENSIVE EVALUATION - BLEU SCORES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Import evaluation functions\n",
    "from inference_evaluation_v2 import (\n",
    "    compute_sentence_bleu,\n",
    "    calculate_corpus_bleu,\n",
    "    calculate_gemini_score,\n",
    "    evaluate_translations,\n",
    "    print_evaluation_results,\n",
    "    translate_sentence\n",
    ")\n",
    "\n",
    "# EVALUATION CONFIGURATION\n",
    "\n",
    "EVAL_CONFIG = {\n",
    "    'beam_size': 5,\n",
    "    'max_length': 200,\n",
    "    'num_samples': 1000,  # None = use all samples from file\n",
    "    'show_examples': 10,  # Number of examples to show\n",
    "    'use_gemini': False,\n",
    "    'gemini_api_key': \"AIzaSyDqHNpCj6xpao90muWoPOSwIBTcNzVk6Is\"\n",
    "}\n",
    "\n",
    "# File paths\n",
    "VI_FILE_PATH = '/kaggle/input/eva-model/kaggle_vi_sents_first1000.txt'\n",
    "EN_FILE_PATH = '/kaggle/input/eva-model/kaggle_en_sents_first1000.txt'\n",
    "\n",
    "print(\"\\n Evaluation Configuration:\")\n",
    "for key, value in EVAL_CONFIG.items():\n",
    "    if key != 'gemini_api_key':\n",
    "        print(f\"  {key:20s}: {value}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# FUNCTION: LOAD SENTENCES FROM FILE\n",
    "\n",
    "def load_sentences_from_file(file_path, max_samples=None):\n",
    "    \"\"\"\n",
    "    Load sentences from text file\n",
    "    \"\"\"\n",
    "    print(f\"\\n Loading sentences from: {file_path}\")\n",
    "    \n",
    "    sentences = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:  # Skip empty lines\n",
    "                sentences.append(line)\n",
    "                if max_samples and len(sentences) >= max_samples:\n",
    "                    break\n",
    "    \n",
    "    print(f\" Loaded {len(sentences)} sentences\")\n",
    "    return sentences\n",
    "\n",
    "# FUNCTION: GENERATE TRANSLATIONS FROM FILE\n",
    "\n",
    "\n",
    "def generate_translations_from_file(model, source_sentences, src_tokenizer, \n",
    "                                   tgt_tokenizer, device, beam_size=5, max_len=100):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    print(f\"\\n Generating translations for {len(source_sentences)} samples...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src_text in tqdm(source_sentences, desc=\"Translating\"):\n",
    "            # Generate translation\n",
    "            pred_text = translate_sentence(\n",
    "                model=model,\n",
    "                sentence=src_text,\n",
    "                src_tokenizer=src_tokenizer,\n",
    "                tgt_tokenizer=tgt_tokenizer,\n",
    "                device=device,\n",
    "                use_beam_search=True,\n",
    "                beam_size=beam_size,\n",
    "                max_len=max_len\n",
    "            )\n",
    "            predictions.append(pred_text)\n",
    "    \n",
    "    print(f\" Generated {len(predictions)} translations\")\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# LOAD DATA FROM FILES\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" LOADING DATA FROM FILES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load Vietnamese sentences\n",
    "vi_sentences = load_sentences_from_file(VI_FILE_PATH, EVAL_CONFIG['num_samples'])\n",
    "\n",
    "# Load English sentences (references)\n",
    "en_sentences = load_sentences_from_file(EN_FILE_PATH, EVAL_CONFIG['num_samples'])\n",
    "\n",
    "# Make sure both files have same number of sentences\n",
    "min_length = min(len(vi_sentences), len(en_sentences))\n",
    "vi_sentences = vi_sentences[:min_length]\n",
    "en_sentences = en_sentences[:min_length]\n",
    "\n",
    "print(f\"\\n Using {min_length} sentence pairs for evaluation\")\n",
    "\n",
    "# EVALUATE VI->EN MODEL\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" EVALUATING VI->EN MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n Loading best VI->EN checkpoint...\")\n",
    "checkpoint_paths_vi_en = [\n",
    "    '/kaggle/input/checkpoint-nlp/best_model_vi_en_14.pt',\n",
    "    f'{OUTPUT_PATH}/checkpoints/vi_en/best_model.pt',\n",
    "    f'{OUTPUT_PATH}/checkpoints/vi_en/latest_checkpoint.pt'\n",
    "]\n",
    "\n",
    "checkpoint_path = None\n",
    "for path in checkpoint_paths_vi_en:\n",
    "    if os.path.exists(path):\n",
    "        checkpoint_path = path\n",
    "        print(f\" Found checkpoint: {path}\")\n",
    "        break\n",
    "\n",
    "if checkpoint_path is None:\n",
    "    raise FileNotFoundError(\"‚ùå No VI->EN checkpoint found!\")\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "model_vi_en.load_state_dict(checkpoint['model_state_dict'])\n",
    "model_vi_en.eval()\n",
    "print(\" Model loaded!\")\n",
    "\n",
    "# Generate translations\n",
    "sources_vi = vi_sentences\n",
    "refs_en = en_sentences\n",
    "preds_en = generate_translations_from_file(\n",
    "    model_vi_en, vi_sentences, vi_tokenizer, en_tokenizer,\n",
    "    device, beam_size=EVAL_CONFIG['beam_size'],\n",
    "    max_len=EVAL_CONFIG['max_length']\n",
    ")\n",
    "\n",
    "# Calculate CORPUS BLEU\n",
    "print(\"\\n Calculating Corpus BLEU scores...\")\n",
    "bleu_results_vi_en = calculate_corpus_bleu(refs_en, preds_en)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" VI->EN RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n Corpus BLEU Score: {bleu_results_vi_en['corpus_bleu']:.2f}\")\n",
    "print(f\"\\n Detailed Metrics:\")\n",
    "print(f\"  BLEU-1: {bleu_results_vi_en['corpus_bleu_1']:.2f}\")\n",
    "print(f\"  BLEU-2: {bleu_results_vi_en['corpus_bleu_2']:.2f}\")\n",
    "print(f\"  BLEU-3: {bleu_results_vi_en['corpus_bleu_3']:.2f}\")\n",
    "print(f\"  BLEU-4: {bleu_results_vi_en['corpus_bleu_4']:.2f}\")\n",
    "print(f\"  Brevity Penalty: {bleu_results_vi_en['brevity_penalty']:.3f}\")\n",
    "print(f\"  Length Ratio: {bleu_results_vi_en['length_ratio']:.3f}\")\n",
    "\n",
    "# Show sample translations\n",
    "if EVAL_CONFIG['show_examples'] > 0:\n",
    "    print(f\"\\n Sample Translations (first {EVAL_CONFIG['show_examples']}):\")\n",
    "    print(\"=\"*70)\n",
    "    for i in range(min(EVAL_CONFIG['show_examples'], len(sources_vi))):\n",
    "        individual_bleu = compute_sentence_bleu(refs_en[i], preds_en[i])\n",
    "        \n",
    "        print(f\"\\n[Example {i+1}] - Sentence BLEU: {individual_bleu['bleu']:.2f}\")\n",
    "        print(f\" Source (VI): {sources_vi[i]}\")\n",
    "        print(f\" Reference:   {refs_en[i]}\")\n",
    "        print(f\" Prediction:  {preds_en[i]}\")\n",
    "        print(f\"    BLEU-1/2/3/4: {individual_bleu['bleu_1']:.1f}/{individual_bleu['bleu_2']:.1f}/{individual_bleu['bleu_3']:.1f}/{individual_bleu['bleu_4']:.1f}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "# Optional: Gemini evaluation\n",
    "if EVAL_CONFIG['use_gemini'] and EVAL_CONFIG['gemini_api_key']:\n",
    "    print(\"\\n  Running Gemini evaluation (sample of 5 translations)...\")\n",
    "    gemini_scores_vi_en = []\n",
    "    for i in range(min(5, len(sources_vi))):\n",
    "        score = calculate_gemini_score(\n",
    "            sources_vi[i], refs_en[i], preds_en[i],\n",
    "            api_key=EVAL_CONFIG['gemini_api_key']\n",
    "        )\n",
    "        gemini_scores_vi_en.append(score)\n",
    "        if score['gemini_score']:\n",
    "            print(f\"  Example {i+1}: Score={score['gemini_score']:.1f}, \"\n",
    "                  f\"Fluency={score['fluency']:.1f}, Adequacy={score['adequacy']:.1f}\")\n",
    "\n",
    "# EVALUATE EN->VI MODEL\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" EVALUATING EN->VI MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n Loading best EN->VI checkpoint...\")\n",
    "checkpoint_paths_en_vi = [\n",
    "    '/kaggle/input/checkpoint-nlp/best_model_en_vi_14.pt',\n",
    "    f'{OUTPUT_PATH}/checkpoints/en_vi/best_model.pt',\n",
    "    f'{OUTPUT_PATH}/checkpoints/en_vi/latest_checkpoint.pt'\n",
    "]\n",
    "\n",
    "checkpoint_path = None\n",
    "for path in checkpoint_paths_en_vi:\n",
    "    if os.path.exists(path):\n",
    "        checkpoint_path = path\n",
    "        print(f\" Found checkpoint: {path}\")\n",
    "        break\n",
    "\n",
    "if checkpoint_path is None:\n",
    "    raise FileNotFoundError(\"‚ùå No EN->VI checkpoint found!\")\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "model_en_vi.load_state_dict(checkpoint['model_state_dict'])\n",
    "model_en_vi.eval()\n",
    "print(\" Model loaded!\")\n",
    "\n",
    "# Generate translations\n",
    "sources_en = en_sentences\n",
    "refs_vi = vi_sentences\n",
    "preds_vi = generate_translations_from_file(\n",
    "    model_en_vi, en_sentences, en_tokenizer, vi_tokenizer,\n",
    "    device, beam_size=EVAL_CONFIG['beam_size'],\n",
    "    max_len=EVAL_CONFIG['max_length']\n",
    ")\n",
    "\n",
    "# Calculate CORPUS BLEU\n",
    "print(\"\\n Calculating Corpus BLEU scores...\")\n",
    "bleu_results_en_vi = calculate_corpus_bleu(refs_vi, preds_vi)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" EN->VI RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nCorpus BLEU Score: {bleu_results_en_vi['corpus_bleu']:.2f}\")\n",
    "print(f\"\\n Detailed Metrics:\")\n",
    "print(f\"  BLEU-1: {bleu_results_en_vi['corpus_bleu_1']:.2f}\")\n",
    "print(f\"  BLEU-2: {bleu_results_en_vi['corpus_bleu_2']:.2f}\")\n",
    "print(f\"  BLEU-3: {bleu_results_en_vi['corpus_bleu_3']:.2f}\")\n",
    "print(f\"  BLEU-4: {bleu_results_en_vi['corpus_bleu_4']:.2f}\")\n",
    "print(f\"  Brevity Penalty: {bleu_results_en_vi['brevity_penalty']:.3f}\")\n",
    "print(f\"  Length Ratio: {bleu_results_en_vi['length_ratio']:.3f}\")\n",
    "\n",
    "# Show examples\n",
    "if EVAL_CONFIG['show_examples'] > 0:\n",
    "    print(f\"\\n Sample Translations (first {EVAL_CONFIG['show_examples']}):\")\n",
    "    print(\"=\"*70)\n",
    "    for i in range(min(EVAL_CONFIG['show_examples'], len(sources_en))):\n",
    "        individual_bleu = compute_sentence_bleu(refs_vi[i], preds_vi[i])\n",
    "        \n",
    "        print(f\"\\n[Example {i+1}] - Sentence BLEU: {individual_bleu['bleu']:.2f}\")\n",
    "        print(f\" Source (EN): {sources_en[i]}\")\n",
    "        print(f\" Reference:   {refs_vi[i]}\")\n",
    "        print(f\" Prediction:  {preds_vi[i]}\")\n",
    "        print(f\"    BLEU-1/2/3/4: {individual_bleu['bleu_1']:.1f}/{individual_bleu['bleu_2']:.1f}/{individual_bleu['bleu_3']:.1f}/{individual_bleu['bleu_4']:.1f}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "# Optional: Gemini evaluation\n",
    "if EVAL_CONFIG['use_gemini'] and EVAL_CONFIG['gemini_api_key']:\n",
    "    print(\"\\n Running Gemini evaluation (sample of 5 translations)...\")\n",
    "    gemini_scores_en_vi = []\n",
    "    for i in range(min(5, len(sources_en))):\n",
    "        score = calculate_gemini_score(\n",
    "            sources_en[i], refs_vi[i], preds_vi[i],\n",
    "            api_key=EVAL_CONFIG['gemini_api_key']\n",
    "        )\n",
    "        gemini_scores_en_vi.append(score)\n",
    "        if score['gemini_score']:\n",
    "            print(f\"  Example {i+1}: Score={score['gemini_score']:.1f}, \"\n",
    "                  f\"Fluency={score['fluency']:.1f}, Adequacy={score['adequacy']:.1f}\")\n",
    "\n",
    "# COMPREHENSIVE EVALUATION SUMMARY\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" COMPREHENSIVE EVALUATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# VI->EN comprehensive evaluation\n",
    "print(\"\\n VI->EN Comprehensive Metrics:\")\n",
    "eval_results_vi_en = evaluate_translations(\n",
    "    sources=sources_vi,\n",
    "    references=refs_en,\n",
    "    hypotheses=preds_en,\n",
    "    use_gemini=EVAL_CONFIG['use_gemini'],\n",
    "    gemini_api_key=EVAL_CONFIG['gemini_api_key'],\n",
    "    use_sacrebleu=True\n",
    ")\n",
    "print_evaluation_results(eval_results_vi_en)\n",
    "\n",
    "# EN->VI comprehensive evaluation\n",
    "print(\"\\n EN->VI Comprehensive Metrics:\")\n",
    "eval_results_en_vi = evaluate_translations(\n",
    "    sources=sources_en,\n",
    "    references=refs_vi,\n",
    "    hypotheses=preds_vi,\n",
    "    use_gemini=EVAL_CONFIG['use_gemini'],\n",
    "    gemini_api_key=EVAL_CONFIG['gemini_api_key'],\n",
    "    use_sacrebleu=True\n",
    ")\n",
    "print_evaluation_results(eval_results_en_vi)\n",
    "\n",
    "# FINAL SUMMARY\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" FINAL EVALUATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n Overall Performance:\")\n",
    "print(f\"\\n  VI->EN:\")\n",
    "print(f\"    Corpus BLEU:    {bleu_results_vi_en['corpus_bleu']:.2f}\")\n",
    "print(f\"    BLEU-4:         {bleu_results_vi_en['corpus_bleu_4']:.2f}\")\n",
    "print(f\"    Brevity Penalty: {bleu_results_vi_en['brevity_penalty']:.3f}\")\n",
    "print(f\"    Test Samples:   {len(preds_en)}\")\n",
    "\n",
    "print(f\"\\n   EN->VI:\")\n",
    "print(f\"    Corpus BLEU:    {bleu_results_en_vi['corpus_bleu']:.2f}\")\n",
    "print(f\"    BLEU-4:         {bleu_results_en_vi['corpus_bleu_4']:.2f}\")\n",
    "print(f\"    Brevity Penalty: {bleu_results_en_vi['brevity_penalty']:.3f}\")\n",
    "print(f\"    Test Samples:   {len(preds_vi)}\")\n",
    "\n",
    "avg_bleu = (bleu_results_vi_en['corpus_bleu'] + bleu_results_en_vi['corpus_bleu']) / 2\n",
    "print(f\"\\n   Average BLEU: {avg_bleu:.2f}\")\n",
    "\n",
    "# Interpretation guide\n",
    "print(\"\\nBLEU Score Interpretation:\")\n",
    "print(\"  < 10:  Almost unusable\")\n",
    "print(\"  10-19: Difficult to understand\")\n",
    "print(\"  20-29: Understandable with effort\")\n",
    "print(\"  30-39: Understandable\")\n",
    "print(\"  40-49: High quality\")\n",
    "print(\"  50-59: Very high quality\")\n",
    "print(\"  > 60:  Native-like quality\")\n",
    "\n",
    "# Quality assessment\n",
    "if avg_bleu < 20:\n",
    "    quality = \" Poor - Needs significant improvement\"\n",
    "elif avg_bleu < 30:\n",
    "    quality = \"  Fair - Understandable but needs work\"\n",
    "elif avg_bleu < 40:\n",
    "    quality = \" Good - Acceptable quality\"\n",
    "elif avg_bleu < 50:\n",
    "    quality = \" Very Good - High quality\"\n",
    "else:\n",
    "    quality = \" Excellent - Professional quality\"\n",
    "\n",
    "print(f\"\\n Overall Quality Assessment: {quality}\")\n",
    "\n",
    "# SAVE RESULTS\n",
    "\n",
    "print(\"\\n Saving evaluation results...\")\n",
    "\n",
    "os.makedirs(f'{OUTPUT_PATH}/results', exist_ok=True)\n",
    "\n",
    "# Save comprehensive results\n",
    "eval_results_full = {\n",
    "    'vi_en': {\n",
    "        'corpus_bleu': bleu_results_vi_en['corpus_bleu'],\n",
    "        'bleu_1': bleu_results_vi_en['corpus_bleu_1'],\n",
    "        'bleu_2': bleu_results_vi_en['corpus_bleu_2'],\n",
    "        'bleu_3': bleu_results_vi_en['corpus_bleu_3'],\n",
    "        'bleu_4': bleu_results_vi_en['corpus_bleu_4'],\n",
    "        'brevity_penalty': bleu_results_vi_en['brevity_penalty'],\n",
    "        'length_ratio': bleu_results_vi_en['length_ratio'],\n",
    "        'num_samples': len(preds_en)\n",
    "    },\n",
    "    'en_vi': {\n",
    "        'corpus_bleu': bleu_results_en_vi['corpus_bleu'],\n",
    "        'bleu_1': bleu_results_en_vi['corpus_bleu_1'],\n",
    "        'bleu_2': bleu_results_en_vi['corpus_bleu_2'],\n",
    "        'bleu_3': bleu_results_en_vi['corpus_bleu_3'],\n",
    "        'bleu_4': bleu_results_en_vi['corpus_bleu_4'],\n",
    "        'brevity_penalty': bleu_results_en_vi['brevity_penalty'],\n",
    "        'length_ratio': bleu_results_en_vi['length_ratio'],\n",
    "        'num_samples': len(preds_vi)\n",
    "    },\n",
    "    'overall': {\n",
    "        'avg_bleu': avg_bleu,\n",
    "        'quality_assessment': quality\n",
    "    },\n",
    "    'config': EVAL_CONFIG,\n",
    "    'data_source': {\n",
    "        'vi_file': VI_FILE_PATH,\n",
    "        'en_file': EN_FILE_PATH\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f'{OUTPUT_PATH}/results/evaluation_results.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(eval_results_full, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Save sample translations\n",
    "with open(f'{OUTPUT_PATH}/results/sample_translations_vi_en.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"VI->EN SAMPLE TRANSLATIONS\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\\n\")\n",
    "    for i in range(min(50, len(sources_vi))):\n",
    "        individual_bleu = compute_sentence_bleu(refs_en[i], preds_en[i])\n",
    "        f.write(f\"[Example {i+1}] BLEU: {individual_bleu['bleu']:.2f}\\n\")\n",
    "        f.write(f\"Source: {sources_vi[i]}\\n\")\n",
    "        f.write(f\"Reference: {refs_en[i]}\\n\")\n",
    "        f.write(f\"Prediction: {preds_en[i]}\\n\")\n",
    "        f.write(f\"BLEU-1/2/3/4: {individual_bleu['bleu_1']:.1f}/{individual_bleu['bleu_2']:.1f}/{individual_bleu['bleu_3']:.1f}/{individual_bleu['bleu_4']:.1f}\\n\")\n",
    "        f.write(\"-\"*70 + \"\\n\\n\")\n",
    "\n",
    "with open(f'{OUTPUT_PATH}/results/sample_translations_en_vi.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"EN->VI SAMPLE TRANSLATIONS\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\\n\")\n",
    "    for i in range(min(50, len(sources_en))):\n",
    "        individual_bleu = compute_sentence_bleu(refs_vi[i], preds_vi[i])\n",
    "        f.write(f\"[Example {i+1}] BLEU: {individual_bleu['bleu']:.2f}\\n\")\n",
    "        f.write(f\"Source: {sources_en[i]}\\n\")\n",
    "        f.write(f\"Reference: {refs_vi[i]}\\n\")\n",
    "        f.write(f\"Prediction: {preds_vi[i]}\\n\")\n",
    "        f.write(f\"BLEU-1/2/3/4: {individual_bleu['bleu_1']:.1f}/{individual_bleu['bleu_2']:.1f}/{individual_bleu['bleu_3']:.1f}/{individual_bleu['bleu_4']:.1f}\\n\")\n",
    "        f.write(\"-\"*70 + \"\\n\\n\")\n",
    "\n",
    "print(\" Results saved!\")\n",
    "print(f\"   - evaluation_results.json\")\n",
    "print(f\"   - sample_translations_vi_en.txt\")\n",
    "print(f\"   - sample_translations_en_vi.txt\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" EVALUATION COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n Output Files:\")\n",
    "print(f\"   JSON results:   {OUTPUT_PATH}/results/evaluation_results.json\")\n",
    "print(f\"   VI->EN samples: {OUTPUT_PATH}/results/sample_translations_vi_en.txt\")\n",
    "print(f\"   EN->VI samples: {OUTPUT_PATH}/results/sample_translations_en_vi.txt\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY METRICS FOR REPORT:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"  VI->EN Corpus BLEU: {bleu_results_vi_en['corpus_bleu']:.2f}\")\n",
    "print(f\"  EN->VI Corpus BLEU: {bleu_results_en_vi['corpus_bleu']:.2f}\")\n",
    "print(f\"  Average BLEU:       {avg_bleu:.2f}\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOWEniCjru+ux1Qa4oBU3gI",
   "gpuType": "T4",
   "mount_file_id": "1RsqYZorWxXjF8iejP8wwOqBGlXbsz1me",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8954871,
     "sourceId": 14111490,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8990646,
     "sourceId": 14113755,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8965188,
     "sourceId": 14120196,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 16545.295924,
   "end_time": "2025-12-12T09:19:00.626767",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-12T04:43:15.330843",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
